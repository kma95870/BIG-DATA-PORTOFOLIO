# ğŸ§  INTRO : RÃ©seaux de Neurones & Deep Learning

---

## ğŸ”¹ 1. Introduction Ã  lâ€™IA, ML et DL

* **IA (Intelligence Artificielle)** : crÃ©er des machines intelligentes.
* **ML (Machine Learning)** : apprentissage automatique Ã  partir des donnÃ©es.
* **DL (Deep Learning)** : sous-domaine du ML basÃ© sur les **rÃ©seaux de neurones profonds**, capables dâ€™apprendre **automatiquement les features**.

â¡ï¸ Le Deep Learning sâ€™inspire du cerveau humain (neurones, synapses, propagation dâ€™un signal).

---

## ğŸ”¹ 2. Le Neurone Artificiel

### Fonctionnement :

[
z = \sum w_i x_i + b
]
[
a = f(z)
]

* **$w_i$** : poids, importance de chaque feature.
* **$b$** : biais, ajuste le seuil de dÃ©clenchement.
* **$f(z)$** : fonction dâ€™activation (sigmoid, tanh, ReLU, softmax).

### Fonctions dâ€™activation :

* **Sigmoid** : probabilitÃ© entre 0 et 1.
* **tanh** : sortie entre -1 et 1.
* **ReLU** : $max(0,z)$, rapide, Ã©vite vanishing gradient.
* **Softmax** : classification multi-classes.

---

## ğŸ”¹ 3. Architecture des RÃ©seaux de Neurones

* **Input layer** : variables dâ€™entrÃ©e.
* **Hidden layers** : transformations non-linÃ©aires.
* **Output layer** : prÃ©diction finale.

â¡ï¸ Les modÃ¨les **profonds** = plusieurs couches cachÃ©es.

---

## ğŸ”¹ 4. Apprentissage dâ€™un RÃ©seau

1. **Forward pass** : calcul des sorties $\hat{y}$.
2. **Fonction de coÃ»t** : ex. MSE (rÃ©gression), Cross-Entropy (classification).
3. **Backpropagation** : calcul des gradients par dÃ©rivation en chaÃ®ne.
4. **Optimisation** : mise Ã  jour des poids via descente de gradient (SGD, Adam, RMSpropâ€¦).

---

## ğŸ”¹ 5. Types de RÃ©seaux de Neurones

### ğŸ”¸ 5.1 ANN (Perceptron multicouche)

* RÃ©seau de neurones dense (**Fully Connected**).
* Base de tout le deep learning.
* UtilisÃ© pour donnÃ©es tabulaires, classification simple.

### ğŸ”¸ 5.2 CNN (Convolutional Neural Networks)

* AdaptÃ©s aux **images, audio, vidÃ©os**.
* **Convolution** : dÃ©tection de motifs (bords, textures, objets).
* **ReLU** : non-linÃ©aritÃ©.
* **Pooling** : rÃ©duction dimensionnelle (max pooling).
* **Flattening + Fully connected** : classification.
* **Softmax + Cross-Entropy** : sortie probabiliste.
  â¡ï¸ Applications : reconnaissance faciale, dÃ©tection dâ€™objets, vision autonome.

### ğŸ”¸ 5.3 RNN (Recurrent Neural Networks)

* GÃ¨rent les **sÃ©quences temporelles** (texte, sÃ©ries financiÃ¨res, audio).
* Chaque Ã©tat dÃ©pend de lâ€™Ã©tat prÃ©cÃ©dent (mÃ©moire courte).
* ProblÃ¨me : **vanishing gradient**.

#### AmÃ©liorations :

* **LSTM (Long Short-Term Memory)** : mÃ©moire longue grÃ¢ce Ã  des **portes** (input, forget, output).
* **GRU (Gated Recurrent Unit)** : plus simple et rapide.

â¡ï¸ Applications : traduction automatique, prÃ©vision boursiÃ¨re, analyse de sentiments.

### ğŸ”¸ 5.4 Transformers

* BasÃ©s sur le mÃ©canisme **dâ€™attention**.
* Permettent de traiter de longues sÃ©quences efficacement.
* Exemples : **BERT, GPT (ChatGPT), T5, ViT**.
  â¡ï¸ Applications : NLP, rÃ©sumÃ© de texte, vision par ordinateur, multimodalitÃ©.

---

## ğŸ”¹ 6. ProblÃ¨mes & Solutions

* **Overfitting** : rÃ©gularisation (Dropout, L2, BatchNorm, Early Stopping).
* **Vanishing gradient** : ReLU, LSTM, normalisation des poids.
* **Besoin massif en donnÃ©es** : Data Augmentation, Transfer Learning.

---

## ğŸ”¹ 7. Optimisation & HyperparamÃ¨tres

* **Learning rate** : vitesse dâ€™apprentissage.
* **Batch size** : nombre dâ€™exemples par mise Ã  jour.
* **Epochs** : nombre de passages sur lâ€™ensemble des donnÃ©es.
* **Optimizers** : SGD, Adam, RMSprop.

---

## ğŸ”¹ 8. Cas pratiques

* **ANN** â†’ prÃ©dire si un client quitte une banque (churn).
* **CNN** â†’ reconnaÃ®tre un chiffre manuscrit (MNIST).
* **RNN/LSTM** â†’ prÃ©dire le cours dâ€™une action boursiÃ¨re.
* **Transformers** â†’ traduire un texte ou gÃ©nÃ©rer du langage naturel.

---

## ğŸ”¹ 9. Exemple pratique (CNN avec Keras)

```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense

model = Sequential([
    Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1)),
    MaxPooling2D(2,2),
    Flatten(),
    Dense(128, activation='relu'),
    Dense(10, activation='softmax')
])

model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])

model.fit(X_train, y_train, epochs=10, batch_size=64, validation_split=0.2)
```

---

## ğŸ”¹ 10. Conclusion

* **ANN** = base.
* **CNN** = vision et perception.
* **RNN/LSTM** = sÃ©quences.
* **Transformers** = Ã©tat de lâ€™art en NLP et multimodalitÃ©.

ğŸ‘‰ Le Deep Learning permet de rÃ©soudre des problÃ¨mes **complexes, multidimensionnels, sÃ©quentiels et perceptuels** en imitant lâ€™intelligence humaine.

