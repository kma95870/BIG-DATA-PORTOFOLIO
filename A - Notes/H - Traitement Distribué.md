

---

# ğŸ“˜ Partie 1 : Fondamentaux du Traitement DistribuÃ©

---

## 1ï¸âƒ£ Introduction au traitement distribuÃ©

### ğŸ“Œ DÃ©finition

Le **traitement distribuÃ©** consiste Ã  exÃ©cuter un programme ou un calcul sur plusieurs machines (appelÃ©es *nÅ“uds*) connectÃ©es en rÃ©seau, afin de **partager la charge** et **accÃ©lÃ©rer le traitement**.

ğŸ‘‰ IdÃ©e clÃ© : *Diviser pour rÃ©gner*.
Un gros problÃ¨me est dÃ©coupÃ© en sous-problÃ¨mes, chacun est traitÃ© indÃ©pendamment, puis les rÃ©sultats sont agrÃ©gÃ©s.

---

### ğŸ¯ Pourquoi en avons-nous besoin ?

1. **Explosion des donnÃ©es** (Big Data).

   * Exemple : Google doit indexer des milliards de pages web.
2. **AccÃ©lÃ©rer les calculs lourds**.

   * Exemple : entraÃ®nement de modÃ¨les IA avec des milliards de paramÃ¨tres.
3. **TolÃ©rance aux pannes**.

   * Exemple : un serveur tombe en panne ? Dâ€™autres prennent le relais.
4. **ScalabilitÃ©**.

   * On ajoute des machines plutÃ´t que dâ€™acheter un seul super-ordinateur :
      * Data Scaling : techniques pour gÃ©rer, stocker et traiter de trÃ¨s grands volumes de donnÃ©es:
      - Vertical Scaling = augmenter la capacitÃ© dâ€™une seule machine (CPU, RAM).
      - Horizontal Scaling = ajouter plusieurs machines (cluster distribuÃ©).

---

### ğŸ”„ DiffÃ©rence avec dâ€™autres approches

* **CentralisÃ©** : tout est traitÃ© sur une seule machine.
* **ParallÃ¨le** : plusieurs processeurs dans une seule machine (ex : CPU multi-cÅ“urs).
* **DistribuÃ©** : plusieurs machines indÃ©pendantes, reliÃ©es par un rÃ©seau.

---

### ğŸ¢ Exemples concrets

* **Google Search** : utilise MapReduce puis Spark pour traiter les requÃªtes.
* **Netflix** : Spark pour recommandations personnalisÃ©es.
* **Uber** : Kafka + Flink pour analyser les trajets GPS en temps rÃ©el.
* **Safran / Airbus** : Spark Streaming pour analyser les capteurs moteurs.

---

## 2ï¸âƒ£ Architecture dâ€™un systÃ¨me distribuÃ©

Un **systÃ¨me distribuÃ©** est composÃ© de plusieurs nÅ“uds qui collaborent.

### ğŸ”§ RÃ´les

* **NÅ“ud maÃ®tre (Master)** : coordonne et rÃ©partit les tÃ¢ches.
* **NÅ“uds travailleurs (Workers)** : exÃ©cutent les calculs.
* **SystÃ¨me de stockage distribuÃ©** : garde les donnÃ©es accessibles Ã  tous (HDFS, S3, etc.).

---

### ğŸ“Š SchÃ©ma simplifiÃ©

```
         +-------------------+
         |    Utilisateur    |
         +---------+---------+
                   |
             [Master Node]
          (coordonne le travail)
             /     |     \
   [Worker 1] [Worker 2] [Worker 3]
       |          |          |
   DonnÃ©es 1   DonnÃ©es 2   DonnÃ©es 3
```

---

### âš¡ CaractÃ©ristiques dâ€™un systÃ¨me distribuÃ©

1. **ScalabilitÃ© horizontale** : on ajoute des machines pour plus de puissance.
2. **TolÃ©rance aux pannes** : si un nÅ“ud Ã©choue, un autre reprend le calcul.
3. **ParallÃ©lisme** : les donnÃ©es ou tÃ¢ches sont traitÃ©es en mÃªme temps.
4. **Communication rÃ©seau** : les nÅ“uds Ã©changent des donnÃ©es.
5. **HÃ©tÃ©rogÃ©nÃ©itÃ©** : les machines peuvent Ãªtre diffÃ©rentes (CPU, mÃ©moire, OS).

---

## 3ï¸âƒ£ ModÃ¨les de parallÃ©lisation

Il existe plusieurs faÃ§ons de rÃ©partir le travail :

1. **Data Parallelism** : mÃªme tÃ¢che sur des morceaux diffÃ©rents de donnÃ©es.
   â†’ Exemple : compter les mots dans 1 000 fichiers.

2. **Task Parallelism** : tÃ¢ches diffÃ©rentes exÃ©cutÃ©es en parallÃ¨le.
   â†’ Exemple : une tÃ¢che calcule la moyenne, une autre calcule la variance.

3. **Pipeline Parallelism** : les Ã©tapes se suivent comme une chaÃ®ne de montage.
   â†’ Exemple : prÃ©traitement â†’ nettoyage â†’ analyse â†’ stockage.

4. **Embarrassingly Parallel** (parallÃ©lisme simple) :
   * Les tÃ¢ches sont faciles Ã  diviser et indÃ©pendantes.
   * Chaque nÅ“ud peut travailler seul sans communiquer avec les autres.
   â†’ Exemple : appliquer la mÃªme fonction Ã  1 000 images.

5. **Non-Embarrassingly Parallel** (parallÃ©lisme complexe) :
   * Les tÃ¢ches doivent communiquer entre elles.
   * Ã‰changes frÃ©quents entre nÅ“uds (rÃ©seau).
   â†’ Exemple : algorithmes oÃ¹ chaque Ã©tape dÃ©pend dâ€™une autre (graphes, ML distribuÃ©).
   * Plus fragile : un seul nÅ“ud en panne peut bloquer le calcul.
---

## 4ï¸âƒ£ TolÃ©rance aux pannes

* **RÃ©plication des donnÃ©es** : chaque fichier est stockÃ© sur plusieurs nÅ“uds.
* **Re-exÃ©cution des tÃ¢ches** : si une machine tombe, une autre reprend.
* **Checkpointing** : sauvegarde intermÃ©diaire des calculs pour Ã©viter de tout recommencer.

---

## 5ï¸âƒ£ Mini TP : ParallÃ©liser un calcul en Python ğŸ

Avant de passer Ã  Hadoop/Spark, voyons un **exemple simple de traitement distribuÃ©** en Python avec le module `multiprocessing`.
But : calculer le carrÃ© de plusieurs nombres **en parallÃ¨le**.

```python
import multiprocessing

# Fonction Ã  exÃ©cuter
def square(x):
    return x * x

if __name__ == "__main__":
    # DonnÃ©es Ã  traiter
    data = [1, 2, 3, 4, 5, 6, 7, 8]

    # CrÃ©ation d'un pool de 4 workers
    pool = multiprocessing.Pool(processes=4)

    # Distribution des tÃ¢ches
    results = pool.map(square, data)

    pool.close()
    pool.join()

    print("RÃ©sultats :", results)
```

ğŸ‘‰ Ici, les 8 calculs sont distribuÃ©s Ã  4 *workers* (processus) â†’ plus rapide que sÃ©quentiel.
Câ€™est le mÃªme principe quâ€™un cluster distribuÃ©, mais Ã  lâ€™Ã©chelle dâ€™un seul ordinateur.

---

## âœ… Conclusion Partie 1

* Le traitement distribuÃ© = **partage du travail entre plusieurs machines**.
* Il repose sur **scalabilitÃ©, parallÃ©lisme et tolÃ©rance aux pannes**.
* On distingue plusieurs modÃ¨les (data, task, pipeline parallelism).
* En pratique, les systÃ¨mes modernes (Hadoop, Spark, Flink) implÃ©mentent ces principes.

---

# ğŸ“˜ Partie 2 : Hadoop et lâ€™Ã¨re MapReduce

---

## 1ï¸âƒ£ Pourquoi Hadoop ?

ğŸ‘‰ ProblÃ¨me de dÃ©part (annÃ©es 2000) :

* Explosion du volume de donnÃ©es (web, logs, capteurs, etc.).
* Besoin dâ€™un systÃ¨me **scalable**, **tolÃ©rant aux pannes** et **peu coÃ»teux** (clusters de machines standards au lieu de supercalculateurs).

ğŸ‘‰ RÃ©ponse : **Apache Hadoop**, inspirÃ© des papiers de Google sur **Google File System (GFS)** et **MapReduce**.

---

## 2ï¸âƒ£ Composants principaux dâ€™Hadoop

Hadoop repose sur **3 briques principales** :

1. **HDFS (Hadoop Distributed File System)** â†’ Stockage distribuÃ©.
2. **MapReduce** â†’ ModÃ¨le de calcul distribuÃ©.
3. **YARN (Yet Another Resource Negotiator)** â†’ Gestionnaire de ressources et dâ€™exÃ©cutions.

---

## 3ï¸âƒ£ HDFS (Hadoop Distributed File System)

### ğŸ“Œ Fonctionnement

* Write Once, Read Many â†’ on ne modifie pas un fichier existant, on ajoute des donnÃ©es nouvelles (append).
* Un fichier est dÃ©coupÃ© en **blocs** (par dÃ©faut 128 Mo).
* Chaque bloc est **rÃ©pliquÃ©** (souvent 3 copies) sur diffÃ©rents nÅ“uds.
* Deux types de nÅ“uds :

  * **NameNode** (maÃ®tre) â†’ tient la carte des fichiers et blocs.
  * **DataNodes** (travailleurs) â†’ stockent rÃ©ellement les blocs.

### ğŸ“Š SchÃ©ma

```
        +----------------+
        |    NameNode    | <-- gÃ¨re la "mÃ©tadonnÃ©e"
        +----------------+
           /    |     \
          /     |      \
 +----------+ +----------+ +----------+
 | DataNode | | DataNode | | DataNode |
 |  Bloc 1  | |  Bloc 2  | |  Bloc 3  |
 +----------+ +----------+ +----------+
```

### âœ… Avantages

* **TolÃ©rance aux pannes** (rÃ©plication).
* **ScalabilitÃ© horizontale**.
* AccÃ¨s distribuÃ© aux donnÃ©es.

---

## 4ï¸âƒ£ MapReduce

### ğŸ“Œ DÃ©finition

Un modÃ¨le de programmation inventÃ© par Google (2004).
Deux Ã©tapes principales :

1. **Map** : appliquer une fonction Ã  chaque Ã©lÃ©ment des donnÃ©es (parallÃ¨le).
2. **Reduce** : agrÃ©ger les rÃ©sultats intermÃ©diaires.

ğŸ‘‰ Exemple : **Word Count** (compter les mots dans des millions de documents).

### ğŸ”„ Exemple logique

Texte dâ€™entrÃ©e :

```
"Alice aime Bob. Bob aime Alice."
```

* **Map** :

```
(Alice, 1), (aime, 1), (Bob, 1), (Bob, 1), (aime, 1), (Alice, 1)
```

* **Shuffle & Sort** (regroupement par clÃ©) :

```
(Alice, [1, 1]), (aime, [1, 1]), (Bob, [1, 1])
```

* **Reduce** :

```
(Alice, 2), (aime, 2), (Bob, 2)
```

---

## 5ï¸âƒ£ YARN (Yet Another Resource Negotiator)

### ğŸ“Œ RÃ´le

* Introduit dans Hadoop 2.0 pour remplacer lâ€™ancien gestionnaire.
* GÃ¨re **les ressources** (CPU, RAM) et **les jobs** sur le cluster.
* Permet Ã  diffÃ©rents frameworks (MapReduce, Spark, Tez, etc.) de coexister sur le mÃªme cluster.

### âš¡ Fonctionnement

* **Resource Manager (RM)** : dÃ©cide quelle tÃ¢che sâ€™exÃ©cute oÃ¹.
* **Node Manager (NM)** : gÃ¨re les ressources de chaque machine.

---

## 6ï¸âƒ£ Avantages et limites de Hadoop MapReduce

âœ… Avantages :

* TrÃ¨s robuste et tolÃ©rant aux pannes.
* ConÃ§u pour traiter du **Big Data massif**.
* Open-source et dÃ©ployable sur des clusters de machines standards.

âŒ Limites :

* **Lent** car Ã©crit/lu sur disque Ã  chaque Ã©tape (I/O coÃ»teux).
* ModÃ¨le de programmation rigide (difficile Ã  coder des algorithmes complexes).
* NÃ©cessitÃ© dâ€™outils complÃ©mentaires (Pig, Hive, Sparkâ€¦).

ğŸ‘‰ Câ€™est cette lenteur qui a poussÃ© Ã  la crÃ©ation de **Apache Spark** (partie 3).

---

## 7ï¸âƒ£ Mini TP : MapReduce en pseudo-code

Pour bien comprendre, voici un petit **MapReduce simulÃ© en Python**.
On fait un **WordCount** distribuÃ©.

```python
from collections import defaultdict

# Ã‰tape MAP
def map_phase(texts):
    mapped = []
    for text in texts:
        for word in text.split():
            mapped.append((word.lower(), 1))
    return mapped

# Ã‰tape SHUFFLE (groupement par clÃ©)
def shuffle_phase(mapped):
    grouped = defaultdict(list)
    for word, count in mapped:
        grouped[word].append(count)
    return grouped

# Ã‰tape REDUCE
def reduce_phase(grouped):
    reduced = {}
    for word, counts in grouped.items():
        reduced[word] = sum(counts)
    return reduced

# Jeu de donnÃ©es
texts = [
    "Alice aime Bob",
    "Bob aime Alice",
    "Alice adore Spark"
]

mapped = map_phase(texts)
grouped = shuffle_phase(mapped)
reduced = reduce_phase(grouped)

print("RÃ©sultat WordCount :", reduced)
```

ğŸ‘‰ RÃ©sultat attendu :

```
{'alice': 3, 'aime': 2, 'bob': 2, 'adore': 1, 'spark': 1}
```

âš¡ Ce code imite le **flux Map â†’ Shuffle â†’ Reduce** dâ€™Hadoop.

---

## âœ… Conclusion Partie 2

* Hadoop a introduit **HDFS** pour le stockage et **MapReduce** pour le calcul distribuÃ©.
* **YARN** permet de gÃ©rer les ressources et dâ€™exÃ©cuter plusieurs frameworks.
* MapReduce a Ã©tÃ© une rÃ©volution mais est devenu limitÃ© par sa lenteur â†’ Spark est nÃ©.

---

# ğŸ“˜ Partie 3 : Spark â€“ le nouveau standard

---

## 1ï¸âƒ£ Introduction Ã  Spark

### ğŸ“Œ Quâ€™est-ce que Spark ?

* Un **moteur de traitement distribuÃ©** open-source (crÃ©Ã© en 2009 Ã  lâ€™universitÃ© de Berkeley).
* NÃ© pour **remplacer MapReduce** : plus rapide, plus flexible, plus facile Ã  utiliser.
* Utilise la **mÃ©moire vive (in-memory computing)** pour accÃ©lÃ©rer les calculs (au lieu de rÃ©Ã©crire sur disque Ã  chaque Ã©tape).
* Compatible avec : Scala, Python (PySpark), Java, R.

ğŸ‘‰ RÃ©sultat : **jusquâ€™Ã  100x plus rapide que Hadoop MapReduce** dans certains cas.

---

### âš¡ Composants principaux

Spark nâ€™est pas quâ€™un moteur, câ€™est tout un **Ã©cosystÃ¨me** :

* **Spark Core** â†’ moteur de base (RDD, DAG, scheduling).
* **Spark SQL** â†’ manipulation de DataFrames et requÃªtes SQL.
* **MLlib** â†’ Machine Learning distribuÃ©.
* **GraphX** â†’ calculs sur graphes.
* **Spark Streaming / Structured Streaming** â†’ traitement de flux en temps rÃ©el.

---

### ğŸ“Š Architecture Spark

Un cluster Spark ressemble Ã  Hadoop/YARN mais avec sa logique propre :

* **Driver** â†’ programme principal qui coordonne tout.
* **Cluster Manager** (YARN, Mesos ou Spark Standalone) â†’ gÃ¨re les ressources.
* **Executors** â†’ processus qui exÃ©cutent les tÃ¢ches sur les workers.

```
        [Driver Program]
        /       |       \
 [Executor 1] [Executor 2] [Executor 3]
       |            |            |
   Task 1.1     Task 2.1     Task 3.1
   Task 1.2     Task 2.2     Task 3.2
```

---

## 2ï¸âƒ£ Spark Core : RDD et DAG

### ğŸ”¹ RDD (Resilient Distributed Dataset)

* Structure **de base** de Spark : collection distribuÃ©e, immuable, tolÃ©rante aux pannes.
* Dataset distribuÃ© sur plusieurs machines.
* Chaque transformation (map, filter, reduce) est appliquÃ©e en parallÃ¨le sur les partitions.
* **RÃ©silient** : assurÃ©e par lâ€™immutabilitÃ© et le DAG (Directed Acyclic Graph).

ğŸ‘‰ Exemple :

```python
rdd = spark.sparkContext.parallelize([1, 2, 3, 4, 5])
```

---

### ğŸ”¹ DAG (Directed Acyclic Graph)

* Spark reprÃ©sente les transformations comme un **graphe orientÃ© acyclique**.
* Permet lâ€™optimisation automatique et la relance en cas de panne.

---

## 3ï¸âƒ£ Transformations et Actions Spark

### âš¡ Transformations (lazy = diffÃ©rÃ©es)

* `map`, `flatMap`, `filter`, `reduceByKey`, `groupByKey`â€¦
  ğŸ‘‰ Ne sâ€™exÃ©cutent pas directement â†’ Spark construit un plan (DAG).

### âš¡ Actions (dÃ©clenchent le calcul)

* `collect()`, `count()`, `take()`, `saveAsTextFile()`â€¦
  ğŸ‘‰ DÃ©clenchent rÃ©ellement lâ€™exÃ©cution du DAG.

---

### ğŸ“ Exemple simple

```python
# Charger un RDD
rdd = spark.sparkContext.parallelize([1, 2, 3, 4, 5])

# Transformation
squared = rdd.map(lambda x: x * x)

# Action (exÃ©cution)
print(squared.collect())  # [1, 4, 9, 16, 25]
```

---

## 4ï¸âƒ£ Spark SQL & DataFrames

### ğŸ“Œ DataFrames

* Abstraction haut-niveau comme Pandas.
* Permet dâ€™Ã©crire des requÃªtes SQL sur des donnÃ©es distribuÃ©es.

ğŸ‘‰ Exemple :

```python
df = spark.read.json("hdfs://data/people.json")
df.printSchema()
df.select("name").show()
df.filter(df["age"] > 21).show()
```

---

### ğŸ“Œ Spark SQL

* Interface SQL pour requÃªter directement les DataFrames.

ğŸ‘‰ Exemple :

```python
df.createOrReplaceTempView("people")
spark.sql("SELECT name, age FROM people WHERE age > 21").show()
```

---

## 5ï¸âƒ£ Spark MLlib (Machine Learning)

* Librairie ML distribuÃ©e.
* Algorithmes : rÃ©gression, classification, clustering, recommandation.
* Pipelines ML (prÃ©traitement â†’ features â†’ modÃ¨le â†’ Ã©valuation).

ğŸ‘‰ Exemple : rÃ©gression logistique distribuÃ©e pour prÃ©dire du churn.

---

## 6ï¸âƒ£ Spark Streaming

### ğŸ“Œ Deux modes

1. **DStreams (legacy)** â†’ micro-batchs.
2. **Structured Streaming (moderne)** â†’ API unifiÃ©e DataFrame pour le temps rÃ©el.

ğŸ‘‰ Exemple : lecture dâ€™un flux Kafka.

```python
df = spark \
  .readStream \
  .format("kafka") \
  .option("kafka.bootstrap.servers", "localhost:9092") \
  .option("subscribe", "topic1") \
  .load()

df.selectExpr("CAST(key AS STRING)", "CAST(value AS STRING)") \
  .writeStream \
  .format("console") \
  .start() \
  .awaitTermination()
```

---

## 7ï¸âƒ£ Exemple pratique complet : WordCount en PySpark

```python
from pyspark.sql import SparkSession

# CrÃ©er session Spark
spark = SparkSession.builder.appName("WordCount").getOrCreate()

# Charger un fichier texte distribuÃ©
text_file = spark.sparkContext.textFile("hdfs://cluster/data/text.txt")

# Transformation
word_counts = (text_file
               .flatMap(lambda line: line.split(" "))
               .map(lambda word: (word, 1))
               .reduceByKey(lambda a, b: a + b))

# Action (collect)
print(word_counts.collect())
```

---

## 8ï¸âƒ£ Avantages et limites de Spark

âœ… Avantages :

* Ultra-rapide (in-memory).
* API riche et facile (SQL, ML, Streaming).
* IntÃ©gration avec Kafka, HDFS, S3, Cassandraâ€¦
* Compatible Python, Scala, Java, R.

âŒ Limites :

* Peut consommer beaucoup de RAM.
* Optimisation parfois complexe (shuffle, partitionnement).
* Pas forcÃ©ment adaptÃ© au **temps rÃ©el ultra faible latence** (Flink est meilleur).

---

## âœ… Conclusion Partie 3

* Spark est devenu le **standard** du traitement distribuÃ©.
* Remplace MapReduce en offrant un moteur **in-memory**, **flexible** et **rapide**.
* Ã‰cosystÃ¨me complet : Core, SQL, MLlib, Streaming.
* Base de beaucoup de projets Big Data modernes (Netflix, Uber, Safran, etc.).

---

# ğŸ“˜ Partie 4 : Alternatives et Extensions

---

## 1ï¸âƒ£ Apache Flink

### ğŸ“Œ DÃ©finition

* Framework de **traitement distribuÃ© en flux (stream-first)**.
* NÃ© en 2015, souvent prÃ©sentÃ© comme le **concurrent direct de Spark Streaming**.

### âš¡ DiffÃ©rences avec Spark

* Spark Structured Streaming = micro-batchs (mini-lots de quelques ms/s).
* Flink = **vrai streaming temps rÃ©el** (event-at-a-time).

ğŸ‘‰ ConsÃ©quence :

* **Spark** = bon compromis pour batch + streaming.
* **Flink** = spÃ©cialisÃ© dans le temps rÃ©el **ultra faible latence**.

### ğŸ¢ Cas dâ€™usage

* Surveillance de capteurs IoT.
* DÃ©tection de fraudes bancaires en temps rÃ©el.
* Monitoring de systÃ¨mes critiques (finance, aÃ©ronautique).

---

## 2ï¸âƒ£ Apache Kafka

### ğŸ“Œ Quâ€™est-ce que Kafka ?

* Un **systÃ¨me de messagerie distribuÃ©** crÃ©Ã© par LinkedIn (2011).
* Permet de gÃ©rer des **flux massifs de donnÃ©es en temps rÃ©el**.

### ğŸ”§ Principes

* **Producer** â†’ envoie des messages.
* **Topic** â†’ file dâ€™attente partitionnÃ©e, persistÃ©e.
* **Consumer** â†’ lit les messages.
* **Broker** â†’ serveur Kafka qui stocke les messages.

### ğŸ“Š SchÃ©ma

```
Producer --> [Kafka Topic PartitionnÃ©] --> Consumer
```

### ğŸ¢ Cas dâ€™usage

* Uber â†’ flux de positions GPS.
* Netflix â†’ suivi des logs utilisateurs.
* Safran â†’ ingestion des donnÃ©es capteurs moteurs.

ğŸ‘‰ Kafka **ne traite pas** les donnÃ©es â†’ il les transporte.
Il est souvent couplÃ© avec Spark ou Flink pour le calcul.

---

## 3ï¸âƒ£ Bases de donnÃ©es distribuÃ©es (NoSQL)

Les systÃ¨mes de stockage distribuÃ©s ne se limitent pas Ã  HDFS ou S3 :
Certaines bases NoSQL sont conÃ§ues pour fonctionner en cluster.

### ğŸ”¹ Cassandra

* Base orientÃ©e colonnes.
* ScalabilitÃ© horizontale Ã©norme (clusters mondiaux).
* UtilisÃ©e par Instagram, Netflix.

### ğŸ”¹ HBase

* InspirÃ©e de Google BigTable.
* Fonctionne au-dessus de HDFS.
* Excellente pour les **grandes Ã©critures/lectures distribuÃ©es**.

### ğŸ”¹ MongoDB (sharding + rÃ©plication)

* Base orientÃ©e documents JSON.
* Peut Ãªtre distribuÃ©e grÃ¢ce au **sharding** (partage horizontal des donnÃ©es).

### ğŸ¢ Cas dâ€™usage

* **Cassandra** â†’ rÃ©seaux sociaux, logs.
* **HBase** â†’ time series, IoT.
* **MongoDB** â†’ applications web modernes, API.

---

## 4ï¸âƒ£ Exemple pratique : Kafka â†’ Spark Streaming

Exemple dâ€™un flux **Kafka â†’ Spark Streaming** qui lit des messages en temps rÃ©el.

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("KafkaSparkExample") \
    .getOrCreate()

# Lecture du flux Kafka
df = spark.readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "localhost:9092") \
    .option("subscribe", "transactions") \
    .load()

# Transformer les donnÃ©es (clÃ©/valeur en texte)
messages = df.selectExpr("CAST(key AS STRING)", "CAST(value AS STRING)")

# Ã‰crire les rÃ©sultats en temps rÃ©el dans la console
query = messages.writeStream \
    .outputMode("append") \
    .format("console") \
    .start()

query.awaitTermination()
```

ğŸ‘‰ Ici :

* Kafka alimente Spark avec un flux de donnÃ©es (ex. transactions bancaires).
* Spark Streaming les traite en temps rÃ©el (fraudes, agrÃ©gations, anomalies).

---

## 5ï¸âƒ£ Avantages et inconvÃ©nients

âœ… Avantages des extensions :

* Kafka : ingestion massive, scalable, rÃ©silient.
* Flink : meilleur temps rÃ©el que Spark.
* NoSQL distribuÃ©es : lecture/Ã©criture ultra rapides.

âŒ InconvÃ©nients :

* Plus complexes Ã  administrer.
* Pas toujours nÃ©cessaires si on reste dans le batch.
* NÃ©cessitent souvent un Ã©cosystÃ¨me complet (Kafka seul ne sert Ã  rien, il faut un consommateur type Spark/Flink).

---

## âœ… Conclusion Partie 4

* **Flink** â†’ temps rÃ©el ultra faible latence.
* **Kafka** â†’ colonne vertÃ©brale des flux Big Data.
* **Bases NoSQL distribuÃ©es** â†’ stockage et accÃ¨s rapides aux donnÃ©es massives.

ğŸ‘‰ Dans beaucoup dâ€™architectures modernes :
`Kafka â†’ Spark/Flink â†’ Stockage (HDFS/S3/NoSQL) â†’ Dashboard (Tableau/Streamlit)`.

---

# ğŸ“˜ Partie 5 : Le Cloud et la Data moderne

---

## 1ï¸âƒ£ Pourquoi le Cloud pour le traitement distribuÃ© ?

### ğŸ“Œ ProblÃ¨me

* Monter et gÃ©rer un cluster Hadoop/Spark **on-premise** (dans lâ€™entreprise) est coÃ»teux et complexe :

  * Acheter et maintenir les serveurs.
  * Configurer Hadoop/Spark, YARN, Kafka, etc.
  * GÃ©rer la sÃ©curitÃ© et la tolÃ©rance aux pannes.

### âš¡ Solution : Cloud

* Le cloud propose des **clusters managÃ©s** (Hadoop/Spark prÃªts Ã  lâ€™emploi).
* Facturation **Ã  lâ€™usage** â†’ pas besoin dâ€™investir dans du matÃ©riel.
* ScalabilitÃ© **quasi infinie** (ajout de nÅ“uds Ã  la demande).

ğŸ‘‰ Câ€™est la norme aujourdâ€™hui : **le Big Data = Spark + Cloud**.

---

## 2ï¸âƒ£ DÃ©ploiement Spark/Hadoop dans le Cloud

### ğŸ”¹ AWS

* **Amazon EMR (Elastic MapReduce)** : cluster Hadoop/Spark managÃ©.
* **Amazon S3** : stockage distribuÃ© objet (remplace HDFS dans le cloud).
* **Glue** : ETL serverless.

### ğŸ”¹ Google Cloud Platform (GCP)

* **Dataproc** : cluster Hadoop/Spark managÃ©.
* **BigQuery** : Data Warehouse serverless, SQL sur donnÃ©es massives.

### ğŸ”¹ Microsoft Azure

* **HDInsight** : cluster Hadoop/Spark.
* **Azure Synapse** : Data Warehouse cloud.

---

## 3ï¸âƒ£ Nouvelles gÃ©nÃ©rations de solutions cloud

### ğŸ“Œ Data Warehouse Cloud (Serverless)

* **BigQuery (GCP)** :

  * Pas de cluster Ã  gÃ©rer.
  * RequÃªtes SQL sur des pÃ©taoctets.

* **Snowflake** :

  * Data Warehouse multi-cloud.
  * OptimisÃ© pour stockage + calcul sÃ©parÃ©s.

* **Amazon Redshift** :

  * Data Warehouse SQL distribuÃ© sur AWS.

ğŸ‘‰ Ici, lâ€™utilisateur **nâ€™a mÃªme plus besoin de gÃ©rer le cluster**.
Tout est abstrait â†’ on Ã©crit du SQL, le cloud gÃ¨re la distribution.

---

## 4ï¸âƒ£ Exemple dâ€™architecture Big Data moderne

```
            [Sources de donnÃ©es]
      IoT, Web logs, Transactions, API
                     |
                 (Kafka)
                     |
      +--------------------------------+
      |         Traitement Cloud       |
      |  - Spark (EMR / Dataproc)      |
      |  - Flink (Dataflow)            |
      +--------------------------------+
                     |
       +----------------------------+
       |  Stockage / Data Warehouse |
       |  - S3 / GCS / ADLS         |
       |  - BigQuery / Snowflake    |
       +----------------------------+
                     |
           [Visualisation / BI]
     Tableau, Power BI, Streamlit, Looker
```

---

## 5ï¸âƒ£ Avantages du cloud

âœ… **ScalabilitÃ©** : cluster ajustable en temps rÃ©el.
âœ… **Pay as you go** : tu paies uniquement ce que tu utilises.
âœ… **Pas dâ€™infrastructure Ã  maintenir**.
âœ… **Multi-services intÃ©grÃ©s** : stockage, IA, BI, orchestration.

---

## 6ï¸âƒ£ InconvÃ©nients du cloud

âŒ **CoÃ»t cachÃ©** : mal optimisÃ©, le cloud peut coÃ»ter cher.
âŒ **DÃ©pendance fournisseur (lock-in)**.
âŒ **SÃ©curitÃ© et gouvernance** â†’ donnÃ©es sensibles (banque, santÃ©).

---

## 7ï¸âƒ£ CI/CD et orchestration dans le cloud

### ğŸ”¹ Airflow

* Orchestrateur de workflows (ETL, ML pipelines).
* Permet de programmer les jobs Spark/BigQuery/Kafka.

### ğŸ”¹ Docker

* Emballe les applications (Spark job, API MLâ€¦) dans des conteneurs.
* DÃ©ploiement portable (cloud, local, Kubernetes).

### ğŸ”¹ Kubernetes (K8s)

* Orchestre des milliers de conteneurs (Docker).
* Spark peut tourner sur Kubernetes â†’ trÃ¨s populaire.

---

## 8ï¸âƒ£ Cas dâ€™usage rÃ©els

* **Netflix** : Spark sur AWS EMR, S3 pour stockage, Tableau pour reporting.
* **Airbus** : analyse des capteurs en temps rÃ©el (Kafka + Spark + GCP).
* **Uber** : Kafka + Flink + Hadoop, migration vers Google Cloud.
* **Bouygues Telecom** (ton cas ğŸ˜‰) : BigQuery comme Data Warehouse, Spark en support.

---

## âœ… Conclusion Partie 5

* Le cloud a **rÃ©volutionnÃ© le traitement distribuÃ©** en le rendant accessible et flexible.
* Deux approches :

  * **Cluster managÃ©** (EMR, Dataproc, HDInsight) â†’ Hadoop/Spark sans maintenance.
  * **Serverless** (BigQuery, Snowflake) â†’ SQL direct, plus de cluster.
* Le futur va vers des **architectures hybrides** : Kafka â†’ Spark/Flink â†’ Data Warehouse cloud â†’ BI.

---



