Parfait üëç
On va construire un **cours complet sur le Machine Learning non supervis√©**.
Je vais suivre une structure p√©dagogique claire :

1. **Introduction & d√©finitions**
2. **Diff√©rences avec l‚Äôapprentissage supervis√©**
3. **Principaux objectifs**
4. **Familles d‚Äôalgorithmes non supervis√©s**

   * Clustering
   * R√©duction de dimensions
   * D√©tection d‚Äôanomalies
   * Association Rules
5. **D√©tails des algorithmes les plus importants**

   * M√©thodes de clustering (K-Means, DBSCAN, Hi√©rarchique‚Ä¶)
   * PCA / t-SNE / UMAP
   * Isolation Forest, One-Class SVM
   * Apriori, FP-Growth
6. **√âvaluation & m√©triques (comment juger un mod√®le sans labels)**
7. **Applications r√©elles**
8. **Exemple pratique en Python (syntaxe de base)**
9. **R√©sum√© & points cl√©s**

---

# üìò Cours : Machine Learning Non Supervis√©

## 1. Introduction

L‚Äô**apprentissage non supervis√©** est une branche du machine learning o√π l‚Äôon entra√Æne des mod√®les **sans disposer de labels** (c‚Äôest-√†-dire sans variable cible √† pr√©dire).
L‚Äôobjectif est de **d√©couvrir des structures cach√©es** ou des **mod√®les sous-jacents** dans les donn√©es.

üëâ Contrairement √† l‚Äôapprentissage supervis√©, il n‚Äôy a pas de ‚Äúbonne r√©ponse‚Äù fournie.

---

## 2. Diff√©rences avec l‚Äôapprentissage supervis√©

| Crit√®re   | Apprentissage supervis√©                  | Apprentissage non supervis√©                    |
| --------- | ---------------------------------------- | ---------------------------------------------- |
| Donn√©es   | Features + labels (entr√©e/sortie connue) | Seulement les features (entr√©e)                |
| But       | Pr√©dire une sortie connue                | D√©couvrir des structures cach√©es               |
| Exemple   | Pr√©dire le prix d‚Äôune maison             | Regrouper des clients par similarit√©           |
| M√©triques | Accuracy, RMSE, AUC, etc.                | Indices internes (silhouette, Davies-Bouldin‚Ä¶) |

---

## 3. Objectifs de l‚Äôapprentissage non supervis√©

* **Regrouper** des objets similaires (clustering).
* **R√©duire la dimensionnalit√©** des donn√©es (simplification, visualisation).
* **D√©tecter les anomalies** ou comportements atypiques.
* **D√©couvrir des associations** ou r√®gles de co-occurrence.

---

## 4. Familles d‚Äôalgorithmes non supervis√©s

### A. Clustering

But : **regrouper les donn√©es** en clusters homog√®nes.

* **K-Means** (partitionnement bas√© sur la distance aux centro√Ødes).
* **Clustering hi√©rarchique** (arbre de regroupement).
* **DBSCAN** (densit√©, d√©tection de formes complexes, robustes aux outliers).

### B. R√©duction de dimensions

But : **projeter les donn√©es** dans un espace de dimension plus faible.

* **PCA (ACP)** : projection lin√©aire maximisant la variance.
* **t-SNE, UMAP** : m√©thodes non lin√©aires de visualisation.

### C. D√©tection d‚Äôanomalies

But : trouver des **points rares ou atypiques**.

* **Isolation Forest** (arbres qui isolent les anomalies).
* **One-Class SVM** (s√©pare la ‚Äúmasse‚Äù de donn√©es normales des anomalies).

### D. R√®gles d‚Äôassociation

But : **extraire des r√®gles du type ‚Äúsi A alors B‚Äù** dans les donn√©es transactionnelles.

* **Apriori**
* **FP-Growth**

---

## 5. Algorithmes en d√©tail

### 1. üîπ K-Means

#### üìñ Principe

* Invent√© par **MacQueen (1967)**.
* Objectif : choisir `k` centres, puis affecter chaque point au centre le plus proche, recalculer les centres jusqu‚Äô√† convergence.
* Param√®tres : `k`, nombre max d‚Äôit√©rations, initialisation.
* Limites : n√©cessite de conna√Ætre `k`, sensible aux outliers, clusters sph√©riques.
* Fonction objectif :

  $$
  J = \sum_{i=1}^k \sum_{x \in C_i} ||x - \mu_i||^2
  $$

  o√π $\mu_i$ est le centro√Øde du cluster $C_i$.

#### ‚öôÔ∏è Param√®tres principaux (`sklearn.cluster.KMeans`)

* `n_clusters` : nombre de clusters (k).
* `init` : m√©thode d‚Äôinitialisation (`'k-means++'` recommand√©, `'random'` possible).
* `n_init` : nombre de fois que K-means sera lanc√© (par d√©faut 10, choisir plus pour robustesse).
* `max_iter` : nombre maximum d‚Äôit√©rations (par d√©faut 300).
* `tol` : tol√©rance de convergence.
* `random_state` : graine al√©atoire (pour reproductibilit√©).
* `algorithm` : `'lloyd'`, `'elkan'` (Elkan plus rapide sur petits datasets).

#### üí° Cas d‚Äôutilisation

* Segmentation clients en marketing.
* Compression d‚Äôimages (quantification de couleurs).
* Pr√©traitement pour d‚Äôautres mod√®les.

#### üêç Exemple Python

```python
from sklearn.cluster import KMeans

kmeans = KMeans(
    n_clusters=4,
    init='k-means++',
    n_init=10,
    max_iter=300,
    tol=1e-4,
    random_state=42,
    algorithm='lloyd'
)

labels = kmeans.fit_predict(X)
```

#### üìä M√©triques associ√©es

* Inertia (valeur de la fonction co√ªt).
* Silhouette score.

---

### 2. üîπ DBSCAN (Density-Based Spatial Clustering of Applications with Noise)

#### üìñ Principe

* Introduit par **Ester et al., 1996**.
* Objectif : regrouper points s‚Äôils ont suffisamment de voisins dans un rayon `eps`.
* Bas√© sur la densit√© : un cluster est une zone dense s√©par√©e par des zones moins denses.
* Param√®tres : `eps`, `min_samples`.
* Avantages : pas besoin de `k`, g√®re les formes arbitraires.
* Limites : choix des param√®tres d√©licat.
* Cat√©gories de points :

  * **Core points** : ‚â• `min_samples` voisins dans un rayon `eps`.
  * **Border points** : voisins de core points.
  * **Noise points** : isol√©s.

#### ‚öôÔ∏è Param√®tres (`sklearn.cluster.DBSCAN`)

* `eps` : rayon de voisinage.
* `min_samples` : nombre minimal de points pour former un cluster.
* `metric` : distance utilis√©e (`'euclidean'`, `'manhattan'`, etc.).
* `algorithm` : `'auto'`, `'ball_tree'`, `'kd_tree'`, `'brute'`.
* `leaf_size` : taille des feuilles (impacte vitesse).
* `n_jobs` : parall√©lisation.

#### üí° Cas d‚Äôutilisation

* D√©tection de fraudes.
* Segmentation g√©ospatiale.
* Clusters de formes non sph√©riques.

#### üêç Exemple Python

```python
from sklearn.cluster import DBSCAN

dbscan = DBSCAN(
    eps=0.5,
    min_samples=5,
    metric='euclidean',
    algorithm='auto',
    leaf_size=30,
    n_jobs=-1
)

labels = dbscan.fit_predict(X)
```

#### üìä M√©triques associ√©es

* Silhouette score.
* Nombre de clusters trouv√©s.

---

### 3. üîπ PCA (Principal Component Analysis)

#### üìñ Principe

* Objectif : diagonalisation de la matrice de covariance ‚Üí nouvelles variables (composantes principales).
* Cr√©e de nouvelles variables (composantes principales) qui maximisent la variance.
* R√©duit la dimensionnalit√© tout en pr√©servant l‚Äôinformation.
* Utilit√© : compression, visualisation 2D/3D.

#### ‚öôÔ∏è Param√®tres (`sklearn.decomposition.PCA`)

* `n_components` : nombre de composantes (peut √™tre un entier, un float (variance √† conserver), `'mle'`).
* `svd_solver` : m√©thode de d√©composition (`'auto'`, `'full'`, `'arpack'`, `'randomized'`).
* `whiten` : normalisation des composantes.
* `random_state`.

#### üí° Cas d‚Äôutilisation

* Compression d‚Äôimages.
* Visualisation en 2D/3D.
* Pr√©traitement pour acc√©l√©rer l‚Äôapprentissage supervis√©.

#### üêç Exemple Python

```python
from sklearn.decomposition import PCA

pca = PCA(
    n_components=2,
    svd_solver='auto',
    whiten=False,
    random_state=42
)

X_reduced = pca.fit_transform(X)
```

---

### 4. üîπ Isolation Forest

#### üìñ Principe

* Chaque donn√©e est isol√©e via un arbre de partition.
* Les anomalies sont isol√©es plus rapidement (profondeur plus faible).
* Utile pour : fraude, d√©fauts industriels.

#### ‚öôÔ∏è Param√®tres (`sklearn.ensemble.IsolationForest`)

* `n_estimators` : nombre d‚Äôarbres.
* `max_samples` : nombre d‚Äô√©chantillons utilis√©s par arbre.
* `contamination` : proportion attendue d‚Äôanomalies.
* `max_features` : nb de features utilis√©es par arbre.
* `bootstrap` : √©chantillonnage avec ou sans remise.
* `n_jobs`, `random_state`.

#### üí° Cas d‚Äôutilisation

* D√©tection de fraude bancaire.
* Maintenance pr√©dictive.

#### üêç Exemple Python

```python
from sklearn.ensemble import IsolationForest

iso = IsolationForest(
    n_estimators=100,
    max_samples='auto',
    contamination=0.1,
    max_features=1.0,
    bootstrap=False,
    random_state=42,
    n_jobs=-1
)

labels = iso.fit_predict(X)
```

---

### 6. üîπ R√®gles d‚Äôassociation (Apriori)

#### üìñ Principe

* Objectif : explorer les ensembles d‚Äôitems fr√©quents et extraire des r√®gles.
* Exemple : ‚Äú80% des clients qui ach√®tent du pain ach√®tent aussi du beurre‚Äù.
* Recherche des ensembles d‚Äôitems fr√©quents.
* G√©n√®re des r√®gles du type :

  * Support : fr√©quence de l‚Äôitemset.
  * Confiance : probabilit√© que B apparaisse sachant A.
  * Lift : mesure d‚Äôind√©pendance (si >1 ‚Üí corr√©lation positive).

#### ‚öôÔ∏è Param√®tres (`mlxtend.frequent_patterns.apriori`)

* `min_support` : support minimal.
* `use_colnames` : renommer les items avec colonnes originales.
* `max_len` : taille max des itemsets.

#### üí° Cas d‚Äôutilisation

* Panier d‚Äôachat (market basket analysis).
* Recommandation de produits.

#### üêç Exemple Python

```python
from mlxtend.frequent_patterns import apriori, association_rules

# Extraction des itemsets fr√©quents
frequent_itemsets = apriori(df, min_support=0.1, use_colnames=True, max_len=None)

# G√©n√©ration des r√®gles
rules = association_rules(frequent_itemsets, metric="lift", min_threshold=1.0)
```

---

### 7. üîπ Clustering hi√©rarchique (Agglomerative Clustering)

#### üìñ Principe


* Construire un **dendrogramme** (arbre).
* **Agglom√©ratif** : on fusionne les points progressivement (bottom-up).
* Choix de la m√©trique de lien :

  * `ward` (minimisation variance).
  * `complete` (distance max).
  * `average` (distance moyenne).

#### ‚öôÔ∏è Param√®tres (`sklearn.cluster.AgglomerativeClustering`)

* `n_clusters` : nombre de clusters (si fix√©).
* `metric` : distance (`'euclidean'`, `'manhattan'`, etc.).
* `linkage` : `'ward'`, `'complete'`, `'average'`, `'single'`.
* `distance_threshold` : permet de couper l‚Äôarbre √† une distance donn√©e au lieu de fixer `n_clusters`.

#### üí° Cas d‚Äôutilisation

* Classification de documents.
* Taxonomies (bio-informatique).

#### üêç Exemple Python

```python
from sklearn.cluster import AgglomerativeClustering

agg = AgglomerativeClustering(
    n_clusters=3,
    affinity='euclidean',
    linkage='ward',
    distance_threshold=None
)

labels = agg.fit_predict(X)
```

---

## 6. √âvaluation & m√©triques

Comment √©valuer sans labels ?

* **Indices internes** :

  * Coefficient de silhouette (mesure de s√©paration des clusters).
  * Indice de Davies-Bouldin.

* **Indices externes** (si labels dispo pour √©valuation seulement) :

  * ARI (Adjusted Rand Index).
  * NMI (Normalized Mutual Information).

---

## 7. Applications r√©elles

* Segmentation clients (marketing).
* D√©tection de fraudes bancaires.
* Compression et visualisation de donn√©es.
* Analyse de r√©seaux sociaux.
* Bio-informatique (g√®nes similaires).
* Syst√®mes de recommandation.

---

## 8. Exemple pratique en Python

```python
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.datasets import load_iris
import matplotlib.pyplot as plt

# Donn√©es
X, _ = load_iris(return_X_y=True)

# Clustering avec K-Means
kmeans = KMeans(n_clusters=3, random_state=42)
labels = kmeans.fit_predict(X)

# R√©duction dimensionnelle pour visualisation
pca = PCA(n_components=2)
X_reduced = pca.fit_transform(X)

# Visualisation
plt.scatter(X_reduced[:,0], X_reduced[:,1], c=labels, cmap='viridis')
plt.title("Clustering K-Means sur Iris")
plt.show()
```

---

## 9. R√©sum√© & points cl√©s

* L‚Äôapprentissage non supervis√© explore les **structures cach√©es** des donn√©es.
* Principales t√¢ches : clustering, r√©duction de dimension, anomalies, r√®gles d‚Äôassociation.
* Algorithmes phares : **K-Means, DBSCAN, PCA, Isolation Forest, Apriori**.
* M√©triques adapt√©es : **silhouette, Davies-Bouldin, ARI, NMI**.
* Applications vari√©es : segmentation, fraude, biologie, recommandation.














