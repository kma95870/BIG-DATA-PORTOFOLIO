# ğŸ§  Deep Learning â€“ Artificial Neural Networks (ANN)

---

## ğŸ“Œ RÃ©fÃ©rences principales

* *Machine Learning A-Z* â€“ Kirill Eremenko, @SuperDataScience
* *Deep Learning A-Z* â€“ Kirill Eremenko, Hadelin de Ponteves, @SuperDataScience
* *Deep Learning â€“ Artificial Intelligence* â€“ Sarah Malaeb

---

## ğŸ”¹ 1. Quâ€™est-ce que le Deep Learning ?

* **Machine Learning (ML)** est un sous-domaine de lâ€™IA.
* **Deep Learning (DL)** est un sous-domaine du ML.
* Il utilise des **rÃ©seaux de neurones artificiels (ANN)** pour simuler la capacitÃ© du cerveau humain Ã  apprendre et prendre des dÃ©cisions.

---

## ğŸ”¹ 2. Limitations du Machine Learning classique

* Extraction de features manuelle, souvent complexe.
* Difficile pour des problÃ¨mes complexes et haute dimension.
* Le Deep Learning apprend automatiquement les reprÃ©sentations pertinentes â†’ rÃ©duit le problÃ¨me de **haute dimensionnalitÃ©**.

---

## ğŸ”¹ 3. Pourquoi "Deep" Learning ?

* Les rÃ©seaux sont dits *profonds* car ils comportent plusieurs **couches cachÃ©es**.
* Lâ€™apprentissage consiste Ã  **mettre Ã  jour les poids** des neurones via une fonction dâ€™optimisation.
* Un neurone calcule :
  [
  z = \sum w_i x_i + b, \quad a = f(z)
  ]

---

## ğŸ”¹ 4. Techniques en Deep Learning

* Extraction de features supervisÃ©e ou non.
* Transformation des donnÃ©es.
* Analyse de motifs.
* Classification.
* PrÃ©diction.

---

## ğŸ”¹ 5. Vocabulaire clÃ© dâ€™un neurone

* **X** : donnÃ©es dâ€™entrÃ©e.
* **W** : poids.
* **b** : biais.
* **z** : somme pondÃ©rÃ©e intermÃ©diaire.
* **Ïƒ** : fonction dâ€™activation.
* **a** : sortie du neurone.

![Vocabulaire Neurone](Image/neuron_vocabularies.png)


---

## ğŸ”¹ 6. RÃ©seau de Neurones Artificiel (ANN)


* RÃ©seau le plus simple.
* Aussi appelÃ© **Perceptron multicouche (MLP)**.
* **Feed-forward** : connexions uniquement en avant (pas de cycle).
* Lâ€™Ã©tat interne est uniquement dÃ©fini par les poids.
* Organisation en **couches** (entrÃ©e â†’ cachÃ©es â†’ sortie).

![Strucutre ANN](Image/ANN_structure.png)

---

## ğŸ”¹ 7. Backpropagation

* **CÅ“ur de tout rÃ©seau de neurones**.
* Sert Ã  calculer les gradients efficacement.
* Ã‰tapes :

  1. Propagation avant (calcul de la sortie).
  2. Calcul de la fonction de coÃ»t (ex : MSE, Cross-Entropy).
  3. Propagation arriÃ¨re (calcul des gradients).
  4. Mise Ã  jour des poids via un optimiseur (SGD, Adamâ€¦).

---

## ğŸ”¹ 8. Le neurone biologique (analogie)

* **Dendrites** : reÃ§oivent le signal.
* **Axon** : transmet le signal.
* **Synapse** : connexion entre axon dâ€™un neurone et dendrites dâ€™un autre.

![Neurone](Image/neuron.png)

ğŸ“Œ Dans lâ€™ANN :

* Dendrites â†’ entrÃ©es (X).
* Axon â†’ sortie (a).
* Synapse â†’ poids (W).

![Neurone Artificiel](Image/artificial_neuron.png)

---

## ğŸ”¹ 9. Fonctions dâ€™activation

![Fonction Activation](Image/activation_function1.png)

1. **Seuil** (step function) â†’ binaire (0 ou 1).
2. **Sigmoid** â†’ utile pour probabilitÃ©s.
   [
   \sigma(z) = \frac{1}{1+e^{-z}}
   ]
3. **Tanh** â†’ valeurs entre -1 et 1.
4. **ReLU (Rectified Linear Unit)** â†’ $f(z)=\max(0,z)$, rapide et efficace.
5. **Softmax** â†’ multi-classes, transforme les scores en probabilitÃ©s.

![Fonction Activation](Image/activation_function2.png)

ğŸ“Œ Recommandations :

* Couches cachÃ©es â†’ ReLU.
* Sortie binaire â†’ Sigmoid.
* Sortie multi-classes â†’ Softmax.

---

## ğŸ”¹ 10. Comment un ANN apprend ?

1. On calcule la sortie Å· pour chaque entrÃ©e X.
2. On compare Ã  la vÃ©ritÃ© terrain Y via une **fonction de coÃ»t**.
3. On ajuste les poids W pour minimiser lâ€™erreur.
4. On rÃ©pÃ¨te sur plusieurs **Ã©poques** (epochs).

![ANN Process](Image/ANN_process.png)

---

## ğŸ”¹ 11. Descente de gradient

* MÃ©thode pour trouver les poids qui minimisent la fonction de coÃ»t.
* **Batch Gradient Descent** : mise Ã  jour aprÃ¨s tout le dataset.
* **Stochastic Gradient Descent (SGD)** : mise Ã  jour aprÃ¨s chaque exemple.
* **Mini-Batch Gradient Descent** : compromis (lot de donnÃ©es).

![Batch Vs Stochastic](BatcVSStochastic.png)

---

## ğŸ”¹ 12. HyperparamÃ¨tres de lâ€™ANN

* **Learning rate** : vitesse dâ€™apprentissage.
* **Batch size** : taille dâ€™un lot dâ€™Ã©chantillons.
* **Nombre dâ€™Ã©poques** : combien de fois on parcourt les donnÃ©es.

---

## ğŸ”¹ 13. Exemple pratique avec Keras (Churn bancaire)

### Ã‰tapes :

1. Charger et prÃ©parer les donnÃ©es.
2. Encoder les variables catÃ©gorielles.
3. SÃ©parer en train/test.
4. Normaliser (feature scaling).
5. Construire le modÃ¨le ANN.
6. Compiler le modÃ¨le.
7. EntraÃ®ner.
8. Ã‰valuer.

### Exemple de code :

```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

# Initialisation
model = Sequential()

# Couche dâ€™entrÃ©e et premiÃ¨re couche cachÃ©e
model.add(Dense(units=6, activation='relu', input_dim=11, kernel_initializer='uniform'))

# DeuxiÃ¨me couche cachÃ©e
model.add(Dense(units=6, activation='relu', kernel_initializer='uniform'))

# Couche de sortie (binaire)
model.add(Dense(units=1, activation='sigmoid', kernel_initializer='uniform'))

# Compilation
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# EntraÃ®nement
model.fit(X_train, y_train, batch_size=32, epochs=50, validation_split=0.2)
```

---

## ğŸ”¹ 14. Optimisation dâ€™un ANN

* **Pruning** : supprimer des neurones inutiles.
* **Regularization** : L1/L2, Dropout â†’ Ã©viter lâ€™overfitting.
* **Hyperparameter tuning** : ajuster learning rate, batch size, nombre de couches, etc.

---

## ğŸ”¹ 15. Keras

* API haut-niveau pour crÃ©er rapidement des rÃ©seaux de neurones.
* Sâ€™appuie sur TensorFlow, CNTK, ou Theano.
* Syntaxe simple, idÃ©ale pour prototyper.

---

# âœ… Conclusion

Les **ANN** sont la base du Deep Learning :

* Ils imitent le fonctionnement du cerveau humain.
* Leur puissance vient de la **profondeur** (couches multiples) et de lâ€™**optimisation** via backpropagation.
* Ils sont flexibles mais adaptÃ©s surtout aux **donnÃ©es tabulaires**.
* Pour images â†’ CNN, pour sÃ©quences â†’ RNN/LSTM/Transformers.


