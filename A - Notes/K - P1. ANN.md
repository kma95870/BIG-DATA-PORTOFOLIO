# ğŸ§  Deep Learning â€“ Artificial Neural Networks (ANN)

---

## ğŸ“Œ RÃ©fÃ©rences principales

* *Machine Learning A-Z* â€“ Kirill Eremenko, @SuperDataScience
* *Deep Learning A-Z* â€“ Kirill Eremenko, Hadelin de Ponteves, @SuperDataScience
* *Deep Learning â€“ Artificial Intelligence* â€“ Sarah Malaeb

---

## ğŸ”¹ 1. Quâ€™est-ce que le Deep Learning ?

* **Machine Learning (ML)** est un sous-domaine de lâ€™IA.
* **Deep Learning (DL)** est un sous-domaine du ML.
* Il utilise des **rÃ©seaux de neurones artificiels (ANN)** pour simuler la capacitÃ© du cerveau humain Ã  apprendre et prendre des dÃ©cisions.

---

## ğŸ”¹ 2. Limitations du Machine Learning classique

* Extraction de features manuelle, souvent complexe.
* Difficile pour des problÃ¨mes complexes et haute dimension.
* Le Deep Learning apprend automatiquement les reprÃ©sentations pertinentes â†’ rÃ©duit le problÃ¨me de **haute dimensionnalitÃ©**.

---

## ğŸ”¹ 3. Pourquoi "Deep" Learning ?

* Les rÃ©seaux sont dits *profonds* car ils comportent plusieurs **couches cachÃ©es**.
* Lâ€™apprentissage consiste Ã  **mettre Ã  jour les poids** des neurones via une fonction dâ€™optimisation.
* Un neurone calcule :
  [
  z = \sum w_i x_i + b, \quad a = f(z)
  ]

---

## ğŸ”¹ 4. Techniques en Deep Learning

* Extraction de features supervisÃ©e ou non.
* Transformation des donnÃ©es.
* Analyse de motifs.
* Classification.
* PrÃ©diction.

---

## ğŸ”¹ 5. Vocabulaire clÃ© dâ€™un neurone

* **X** : donnÃ©es dâ€™entrÃ©e.
* **W** : poids.
* **b** : biais.
* **z** : somme pondÃ©rÃ©e intermÃ©diaire.
* **Ïƒ** : fonction dâ€™activation.
* **a** : sortie du neurone.

![Vocabulaire Neurone](Image/neuron_vocabularies.png)


---

## ğŸ”¹ 6. RÃ©seau de Neurones Artificiel (ANN)


* RÃ©seau le plus simple.
* Aussi appelÃ© **Perceptron multicouche (MLP)**.
* **Feed-forward** : connexions uniquement en avant (pas de cycle).
* Lâ€™Ã©tat interne est uniquement dÃ©fini par les poids.
* Organisation en **couches** (entrÃ©e â†’ cachÃ©es â†’ sortie).

![Strucutre ANN](Image/ANN_structure.png)

---

## ğŸ”¹ 7. Backpropagation

* **CÅ“ur de tout rÃ©seau de neurones**.
* Sert Ã  calculer les gradients efficacement.
* Ã‰tapes :

  1. Propagation avant (calcul de la sortie).
  2. Calcul de la fonction de coÃ»t (ex : MSE, Cross-Entropy).
  3. Propagation arriÃ¨re (calcul des gradients).
  4. Mise Ã  jour des poids via un optimiseur (SGD, Adamâ€¦).

---

## ğŸ”¹ 8. Le neurone biologique (analogie)

* **Dendrites** : reÃ§oivent le signal.
* **Axon** : transmet le signal.
* **Synapse** : connexion entre axon dâ€™un neurone et dendrites dâ€™un autre.

![Neurone](Image/neuron.png)

ğŸ“Œ Dans lâ€™ANN :

* Dendrites â†’ entrÃ©es (X).
* Axon â†’ sortie (a).
* Synapse â†’ poids (W).

![Neurone Artificiel](Image/artificial_neuron.png)

---

## ğŸ”¹ 9. Fonctions dâ€™activation

![Fonction Activation](Image/activation_function1.png)

### ğŸ”¸ 9.1. Pourquoi a-t-on besoin dâ€™une fonction dâ€™activation ?

Sans fonction dâ€™activation, un rÃ©seau de neurones ne serait quâ€™une **simple combinaison linÃ©aire** des entrÃ©es.

Chaque neurone ferait :
[
z = \sum w_i x_i + b
]
et la sortie serait directement $a = z$.

â¡ï¸ MÃªme si tu empiles plusieurs couches, tu obtiendras toujours une **fonction linÃ©aire**, Ã©quivalente Ã  une **rÃ©gression linÃ©aire**.

Or, la plupart des phÃ©nomÃ¨nes du monde rÃ©el sont **non linÃ©aires** (ex : reconnaissance dâ€™images, langage, sons).
Les fonctions dâ€™activation permettent donc dâ€™introduire une **non-linÃ©aritÃ©** indispensable pour que le rÃ©seau puisse :

* Apprendre des motifs complexes,
* Combiner des variables de maniÃ¨re non triviale,
* DÃ©cider si un neurone doit â€œsâ€™activerâ€ ou non.

---

### ğŸ”¸ 9.2. RÃ´le concret dâ€™une fonction dâ€™activation

Elle transforme la sortie brute du neurone ($z$) en une valeur â€œactivÃ©eâ€ ($a$) :
[
a = f(z)
]

Son rÃ´le est de :
âœ… **Introduire de la non-linÃ©aritÃ©** (rendre le modÃ¨le plus puissant),
âœ… **Stabiliser ou borner les sorties** (ex : entre 0 et 1),
âœ… **DÃ©cider lâ€™activation du neurone**,
âœ… **RÃ©guler la rÃ©tropropagation** (via la dÃ©rivÃ©e de $f$).

![Fonction Activation](Image/activation_function2.png)

ğŸ“Œ Recommandations :

* Couches cachÃ©es â†’ ReLU.
* Sortie binaire â†’ Sigmoid.
* Sortie multi-classes â†’ Softmax.
---

### ğŸ”¸ 9.3. Fonction de Seuil (Step Function)

PremiÃ¨re fonction utilisÃ©e historiquement (Perceptron de Rosenblatt) :

[
f(z) =
\begin{cases}
1 & \text{si } z > 0 \
0 & \text{sinon}
\end{cases}
]

ğŸ“˜ **InterprÃ©tation** : le neurone sâ€™active seulement si le signal dÃ©passe un certain seuil.

âš ï¸ **Limite** :

* Non dÃ©rivable â†’ inutilisable pour la backpropagation.
* DÃ©cision trop brutale (0 ou 1).

---

### ğŸ”¸ 9.4. SigmoÃ¯de

[
f(z) = \frac{1}{1 + e^{-z}}
]

ğŸ“˜ **Effet** : compresse toute valeur rÃ©elle dans lâ€™intervalle (0, 1).
IdÃ©ale pour reprÃ©senter une **probabilitÃ©**.

ğŸ“Š **DÃ©rivÃ©e** :
[
f'(z) = f(z)(1 - f(z))
]

âœ… **Avantages** :

* InterprÃ©tation probabiliste.
* Lisse et diffÃ©rentiable.

âš ï¸ **InconvÃ©nients** :

* **Vanishing gradient** : les gradients deviennent trÃ¨s faibles pour $|z|$ grands.
* CentrÃ©e sur 0.5 â†’ apprentissage lent.

ğŸ“ **Utilisation typique** : sortie binaire (ex : churn / non churn).

---

### ğŸ”¸ 9.5. Tangente Hyperbolique (tanh)

[
f(z) = \tanh(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}
]

ğŸ“˜ **Effet** : valeurs entre -1 et 1.
Plus â€œcentrÃ©eâ€ que la sigmoÃ¯de (valeurs moyennes autour de 0 â†’ plus stable).

âœ… **Avantages** :

* Sorties centrÃ©es â†’ apprentissage plus rapide.

âš ï¸ **InconvÃ©nients** :

* Vanishing gradient pour valeurs extrÃªmes.

ğŸ“ **Utilisation typique** : couches cachÃ©es dans petits rÃ©seaux ou RNN.

---

### ğŸ”¸ 9.6. ReLU (Rectified Linear Unit)

[
f(z) = \max(0, z)
]

ğŸ“˜ **Principe** :

* Si $z > 0$, le neurone transmet sa valeur.
* Si $z \le 0$, il est â€œÃ©teintâ€ (sortie = 0).

âœ… **Avantages** :

* Calcul trÃ¨s rapide.
* AttÃ©nue le problÃ¨me du vanishing gradient.
* Rend le rÃ©seau plus profond et plus stable.

âš ï¸ **InconvÃ©nients** :

* **Dying ReLU problem** : certains neurones restent bloquÃ©s Ã  0 si les poids deviennent trop nÃ©gatifs.

ğŸ“ **Utilisation typique** : toutes les couches cachÃ©es des rÃ©seaux profonds (CNN, ANN, etc.).

---

### ğŸ”¸ 9.7. Leaky ReLU

[
f(z) =
\begin{cases}
z & \text{si } z > 0 \
0.01z & \text{sinon}
\end{cases}
]

ğŸ“˜ **Principe** :
Corrige le â€œdying ReLUâ€ en gardant un petit flux dâ€™information mÃªme quand $z<0$.

âœ… **Avantages** :

* Apprentissage plus fluide.
* ZÃ©ro neurone bloquÃ©.

ğŸ“ **Utilisation typique** : couches cachÃ©es (alternative Ã  ReLU).

---

### ğŸ”¸ 9.8. Softmax

[
f(z_i) = \frac{e^{z_i}}{\sum_j e^{z_j}}
]

ğŸ“˜ **Principe** : transforme un vecteur de scores $(z_1, z_2, ..., z_n)$ en **probabilitÃ©s normalisÃ©es** dont la somme vaut 1.

âœ… **Avantages** :

* InterprÃ©tation directe en probabilitÃ©.
* IdÃ©ale pour multi-classes.

âš ï¸ **InconvÃ©nients** :

* Sensible aux valeurs extrÃªmes de $z$.

ğŸ“ **Utilisation typique** : couche de sortie des modÃ¨les de classification multi-classes (ex : reconnaissance dâ€™images).

---

### ğŸ”¸ 9.9. En rÃ©sumÃ© â€“ Choix de la fonction dâ€™activation

| Type de couche       | Fonction dâ€™activation recommandÃ©e | Cas dâ€™usage                       |
| -------------------- | --------------------------------- | --------------------------------- |
| Couches cachÃ©es      | ReLU / Leaky ReLU / tanh          | Apprentissage profond             |
| Sortie binaire       | Sigmoid                           | Classification binaire            |
| Sortie multi-classe  | Softmax                           | Classification dâ€™images / texte   |
| RÃ©seaux rÃ©currents   | tanh / ReLU                       | SÃ©quences temporelles             |
| RÃ©seaux peu profonds | tanh                              | ModÃ¨les simples / petits datasets |

---

### ğŸ”¸ 9.10. Illustration (Ã  ajouter en images)

Je te recommande dâ€™intÃ©grer ces **visuels explicatifs** dans ton Markdown :

1. **Courbes dâ€™activation :**

   * Axe X = $z$
   * Axe Y = $f(z)$
   * Tracer Sigmoid, tanh, ReLU et Leaky ReLU sur le mÃªme graphique.

2. **SchÃ©ma dâ€™un neurone avec activation :**

   ```
   EntrÃ©es â†’ Somme pondÃ©rÃ©e â†’ Fonction dâ€™activation â†’ Sortie
   ```

3. **Tableau de comparaison** (comme ci-dessus) Ã  afficher sous forme dâ€™image.

---

## ğŸ”¹ 10. Comment un ANN apprend ?

1. On calcule la sortie Å· pour chaque entrÃ©e X.
2. On compare Ã  la vÃ©ritÃ© terrain Y via une **fonction de coÃ»t**.
3. On ajuste les poids W pour minimiser lâ€™erreur.
4. On rÃ©pÃ¨te sur plusieurs **Ã©poques** (epochs).

![ANN Process](Image/ANN_process.png)

---

## ğŸ”¹ 11. Descente de gradient

* MÃ©thode pour trouver les poids qui minimisent la fonction de coÃ»t.
* **Batch Gradient Descent** : mise Ã  jour aprÃ¨s tout le dataset.
* **Stochastic Gradient Descent (SGD)** : mise Ã  jour aprÃ¨s chaque exemple.
* **Mini-Batch Gradient Descent** : compromis (lot de donnÃ©es).

![Batch Vs Stochastic](BatcVSStochastic.png)

---

## ğŸ”¹ 12. HyperparamÃ¨tres de lâ€™ANN

* **Learning rate** : vitesse dâ€™apprentissage.
* **Batch size** : taille dâ€™un lot dâ€™Ã©chantillons.
* **Nombre dâ€™Ã©poques** : combien de fois on parcourt les donnÃ©es.

---

## ğŸ”¹ 13. Exemple pratique avec Keras (Churn bancaire)

### Ã‰tapes :

1. Charger et prÃ©parer les donnÃ©es.
2. Encoder les variables catÃ©gorielles.
3. SÃ©parer en train/test.
4. Normaliser (feature scaling).
5. Construire le modÃ¨le ANN.
6. Compiler le modÃ¨le.
7. EntraÃ®ner.
8. Ã‰valuer.

### Exemple de code :

```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

# Initialisation
model = Sequential()

# Couche dâ€™entrÃ©e et premiÃ¨re couche cachÃ©e
model.add(Dense(units=6, activation='relu', input_dim=11, kernel_initializer='uniform'))

# DeuxiÃ¨me couche cachÃ©e
model.add(Dense(units=6, activation='relu', kernel_initializer='uniform'))

# Couche de sortie (binaire)
model.add(Dense(units=1, activation='sigmoid', kernel_initializer='uniform'))

# Compilation
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# EntraÃ®nement
model.fit(X_train, y_train, batch_size=32, epochs=50, validation_split=0.2)
```

---

## ğŸ”¹ 14. Optimisation dâ€™un ANN

* **Pruning** : supprimer des neurones inutiles.
* **Regularization** : L1/L2, Dropout â†’ Ã©viter lâ€™overfitting.
* **Hyperparameter tuning** : ajuster learning rate, batch size, nombre de couches, etc.

---

## ğŸ”¹ 15. Keras

* API haut-niveau pour crÃ©er rapidement des rÃ©seaux de neurones.
* Sâ€™appuie sur TensorFlow, CNTK, ou Theano.
* Syntaxe simple, idÃ©ale pour prototyper.

---

# âœ… Conclusion

Les **ANN** sont la base du Deep Learning :

* Ils imitent le fonctionnement du cerveau humain.
* Leur puissance vient de la **profondeur** (couches multiples) et de lâ€™**optimisation** via backpropagation.
* Ils sont flexibles mais adaptÃ©s surtout aux **donnÃ©es tabulaires**.
* Pour images â†’ CNN, pour sÃ©quences â†’ RNN/LSTM/Transformers.


