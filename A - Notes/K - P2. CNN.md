# ğŸ§  Deep Learning â€“ Convolutional Neural Networks (CNN)

---

## ğŸ“š RÃ©fÃ©rences principales

* *Machine Learning A-Z* â€” Kirill Eremenko, Hadelin de Ponteves (@SuperDataScience)
* *Deep Learning â€“ Artificial Intelligence* â€” Sarah Malaeb

---

## ğŸ”¹ 1. Introduction aux CNN

Un **Convolutional Neural Network (CNN)** est un type de rÃ©seau de neurones artificiels conÃ§u pour **traiter des donnÃ©es multidimensionnelles**, en particulier des **images**.

![CNN](./images/CNN.png)


Les CNN sont extrÃªmement efficaces pour :

* la **classification dâ€™images**,
* la **dÃ©tection dâ€™objets**,
* la **segmentation**,
* et dâ€™autres tÃ¢ches de **vision par ordinateur**.

![CNN example](./images/CNN_example.png)

---

## ğŸ”¹ 2. Pourquoi utiliser des CNN ?

Contrairement Ã  un ANN classique :

* Les CNN exploitent la **structure spatiale** des donnÃ©es (les pixels voisins dâ€™une image sont corrÃ©lÃ©s).
* Ils apprennent automatiquement des **caractÃ©ristiques locales** grÃ¢ce aux **filtres de convolution**.

ğŸ“˜ En rÃ©sumÃ© :

> Un CNN est un rÃ©seau de neurones capable dâ€™identifier des motifs visuels (formes, contours, textures) dans les donnÃ©es dâ€™entrÃ©e.

---

## ğŸ”¹ 3. Domaines dâ€™application

Bien que les CNN soient conÃ§us pour les **images**, ils peuvent aussi Ãªtre utilisÃ©s pour :

* ğŸï¸ **VidÃ©os** : reconnaissance dâ€™actions, dÃ©tection dâ€™objets en mouvement.
* ğŸ§  **NLP (texte)** : classification de phrases, analyse de sentiment.
* ğŸµ **Audio** : reconnaissance vocale, classification musicale.
* ğŸ“ˆ **SÃ©ries temporelles** : dÃ©tection dâ€™anomalies, prÃ©visions.

---

## ğŸ”¹ 4. ReprÃ©sentation des images

Avant toute convolution, il faut comprendre comment un **ordinateur â€œvoitâ€ une image** ğŸ‘ï¸â€ğŸ—¨ï¸

### ğŸ”¸ 4.1. Image = matrice de pixels

Une **image numÃ©rique** est une **matrice de valeurs** oÃ¹ chaque Ã©lÃ©ment correspond Ã  lâ€™**intensitÃ© lumineuse** dâ€™un pixel.

![Image binaire](./images/Image_binary.png)

Exemples :

* ğŸ–¤ **Image en niveaux de gris (grayscale)** â†’ matrice 2D
  [
  I(x,y)
  ]
  Chaque pixel a une valeur entre **0 (noir)** et **255 (blanc)**.

  ![Image 2D](./images/image_2D.png)

* ğŸ¨ **Image couleur (RGB)** â†’ matrice 3D
  [
  I(x,y,c)
  ]
  oÃ¹ $c \in {R,G,B}$ (Rouge, Vert, Bleu).
  Ainsi, une image de 64Ã—64 pixels possÃ¨de une matrice de taille **64Ã—64Ã—3**.

  ![Image 3D](./images/image_3D.png)

---

### ğŸ”¸ 4.2. Exemple

Pour un pixel rouge pur :

* R = 255
* G = 0
* B = 0

ğŸ“Š Exemple visuel (pour une image 2Ã—2 couleur) :

| Pixel | R   | G   | B   |
| ----- | --- | --- | --- |
| (1,1) | 255 | 0   | 0   |
| (1,2) | 0   | 255 | 0   |
| (2,1) | 0   | 0   | 255 |
| (2,2) | 255 | 255 | 0   |

---

### ğŸ”¸ 4.3. Image = tenseur

En deep learning, une image est stockÃ©e sous forme de **tenseur** (matrice multidimensionnelle).
Exemple :

* Image couleur : `shape = (hauteur, largeur, canaux)`
* Batch dâ€™images : `shape = (nombre_images, hauteur, largeur, canaux)`

ğŸ“˜ Exemple concret :

```
Image RGB : (64, 64, 3)
Batch de 100 images : (100, 64, 64, 3)
```

---

### ğŸ”¸ 4.4. Normalisation des pixels

Avant dâ€™envoyer une image dans un CNN, on **normalise les valeurs de pixels** :
[
I_{norm} = \frac{I}{255}
]
Cela ramÃ¨ne les valeurs entre **0 et 1**, ce qui :

* stabilise la descente de gradient,
* accÃ©lÃ¨re la convergence,
* Ã©vite des valeurs trop grandes pour la backpropagation.

---

## ğŸ”¹ 5. Structure gÃ©nÃ©rale dâ€™un CNN

Un CNN comprend gÃ©nÃ©ralement plusieurs **types de couches** :

1. **Couche dâ€™entrÃ©e (Input Layer)** : reÃ§oit les donnÃ©es brutes (ex : une image 28x28x3).
2. **Couche de convolution (Convolutional Layer)** : extrait les features via des filtres.
3. **Couche dâ€™activation (ReLU)** : introduit la non-linÃ©aritÃ©.
4. **Couche de pooling** : rÃ©duit la dimension spatiale.
5. **Flattening** : transforme les matrices en vecteur 1D.
6. **Couches entiÃ¨rement connectÃ©es (Fully Connected Layers)** : classification.
7. **Couche de sortie (Output Layer)** : produit la prÃ©diction finale.

![CNN structure](./images/CNN_structure.png)

---

## ğŸ”¸ 5.1 Ã‰tape 1 â€” Convolution

### ğŸ”¹ Principe

La **convolution** consiste Ã  faire glisser un **filtre (kernel)** sur lâ€™image pour en extraire des motifs.

[
\text{Feature map} = \text{Image} * \text{Filtre}
]

![Convultion Example](./images/Convultion_Example.png)

Chaque **filtre** dÃ©tecte un motif spÃ©cifique :

* bords,
* textures,
* contours,
* zones lumineuses, etc.

### ğŸ§® Exemple :

* Image : matrice de pixels (28Ã—28).
* Filtre : matrice 3Ã—3.
* Ã€ chaque position â†’ produit scalaire entre le filtre et la zone couverte â†’ valeur de sortie.

![Convultion Start](./images/Convultion_Step_Start.png)

ğŸ“˜ RÃ©sultat : une **Feature Map**, qui indique oÃ¹ le motif est dÃ©tectÃ©.

![Convultion End](./images/Convultion_Step_End.png)
---

### âœ… Avantages :

* RÃ©duction du nombre de paramÃ¨tres (poids partagÃ©s).
* CapacitÃ© Ã  dÃ©tecter des motifs **indÃ©pendamment de leur position**.

### âš ï¸ InconvÃ©nients :

* Perte dâ€™information lors de la rÃ©duction dimensionnelle.

---

### ğŸ’¡ Pour limiter cette perte :

* A travers l'entraÃ®nement, nous dÃ©cidons quels **features** sont les plus importants.
* On utilise **plusieurs filtres** â†’ plusieurs **feature maps**.
* Chaque filtre apprend un **motif diffÃ©rent** pendant lâ€™entraÃ®nement.

![Convultion Loss](./images/Convultion_Loss.png)

---

#### ğŸ§© 5.1.1 DÃ©finition mathÃ©matique

La **convolution** est une opÃ©ration mathÃ©matique qui combine deux fonctions :

* une **entrÃ©e** ( f(t) ) (ex : une image, un signal ou une sÃ©rie de donnÃ©es),
* et un **filtre** ou **noyau** ( g(t) ) (ex : un dÃ©tecteur de motifs).

Elle est dÃ©finie comme :

[
(f * g)(t) = \int_{-\infty}^{+\infty} f(\tau), g(t - \tau), d\tau
]

ğŸ“˜ Cette intÃ©grale mesure la **similaritÃ©** entre la fonction dâ€™entrÃ©e ( f ) et une version dÃ©calÃ©e du filtre ( g ).

> Autrement dit, ( g ) â€œglisseâ€ sur ( f ) et calcule leur recouvrement (produit scalaire) pour chaque position ( t ).

---

#### ğŸ§  5.1.2 InterprÃ©tation intuitive

Dans un **CNN**, la convolution permet de **dÃ©tecter des motifs** (features) dans une image, comme :

* des bords,
* des textures,
* des formes ou couleurs spÃ©cifiques.

â¡ï¸ Le filtre ( g ) agit comme une **loupe** qui â€œscanneâ€ lâ€™image ( f ).
Ã€ chaque position, il multiplie localement les valeurs et additionne le tout :
â†’ Cela produit une nouvelle **feature map** qui indique **oÃ¹** un certain motif apparaÃ®t.

---

#### âš™ï¸ 5.1.3 Signification des variables

| Symbole        | Signification                              |
| -------------- | ------------------------------------------ |
| ( f )          | Fonction dâ€™entrÃ©e (image, signal original) |
| ( g )          | Filtre ou noyau de convolution             |
| ( t )          | Position de dÃ©calage du filtre             |
| ( \tau )       | Variable dâ€™intÃ©gration / dÃ©calage interne  |
| ( (f * g)(t) ) | RÃ©sultat de la convolution (feature map)   |

---

#### ğŸ”¢ 5.1.4 Forme discrÃ¨te (cas des images numÃ©riques)

Dans le cas dâ€™images (qui sont des matrices de pixels), la convolution devient une **somme discrÃ¨te** :

[
S(i, j) = \sum_m \sum_n I(i - m, j - n) , K(m, n)
]

oÃ¹ :

* ( I ) = image dâ€™entrÃ©e,
* ( K ) = kernel (filtre),
* ( S(i,j) ) = pixel de la **feature map de sortie**.

Chaque valeur de ( S ) est donc une **combinaison locale** des pixels voisins dans ( I ), pondÃ©rÃ©e par les coefficients du filtre ( K ).

---

#### ğŸ§­ 5.1.5 InterprÃ©tation : â€œmodifier la forme de lâ€™autre fonctionâ€

La phrase :

> â€œOne function modifies the shape of the otherâ€

signifie que le **filtre** ( g ) transforme localement la forme de lâ€™entrÃ©e ( f ).
Chaque type de filtre produit un effet diffÃ©rent :

| Type de filtre            | Effet sur lâ€™image     |
| ------------------------- | --------------------- |
| DÃ©tecteur de bords        | Souligne les contours |
| Floutage (Gaussian blur)  | Adoucit les zones     |
| Rehaussement de contraste | Accentue les dÃ©tails  |

Le CNN apprend **automatiquement** ces filtres pendant lâ€™entraÃ®nement.

---

#### ğŸ§® 5.1.6 Vue vectorisÃ©e (formules matricielles)

Les formules Ã  droite de ton image reprÃ©sentent la **convolution vectorisÃ©e** utilisÃ©e dans la rÃ©tropropagation (backpropagation).

Elles expriment la **dÃ©rivÃ©e du rÃ©sultat** ( z ) de la convolution par rapport Ã  :

* lâ€™entrÃ©e ( Y ),
* et les poids du filtre ( F ).

Exemple :

[
\frac{\partial z}{\partial(\text{vec}(y)^T)}(F^T \otimes I) = (F \otimes I)\frac{\partial z}{\partial \text{vec}(y)}
]

ğŸ“˜ Cela permet de **calculer les gradients** pour **ajuster les poids du filtre** lors de lâ€™entraÃ®nement du CNN â€” câ€™est le cÅ“ur du **deep learning**.

---

#### ğŸ“Š 5.1.7 RÃ©sumÃ© visuel

| Ã‰lÃ©ment           | InterprÃ©tation                          |
| ----------------- | --------------------------------------- |
| ( f )             | EntrÃ©e (image ou signal)                |
| ( g )             | Filtre (noyau)                          |
| ( * )             | OpÃ©ration de convolution                |
| ( (f * g)(t) )    | RÃ©sultat (feature map)                  |
| IntÃ©grale / somme | Produit scalaire entre entrÃ©e et filtre |

---

## ğŸ”¸ 5.2 Ã‰tape 1â€™ â€” ReLU Layer (Rectified Linear Unit)

AprÃ¨s chaque convolution, on applique une **fonction dâ€™activation ReLU** :
[
f(x) = \max(0, x)
]

### ğŸ¯ Pourquoi ?

* Pour **introduire la non-linÃ©aritÃ©** (les images sont des donnÃ©es non linÃ©aires).
* Pour **supprimer les valeurs nÃ©gatives** issues de la convolution.

ğŸ“Š **Avant ReLU** : valeurs positives et nÃ©gatives.
ğŸ“Š **AprÃ¨s ReLU** : seulement des valeurs positives â†’ simplifie la propagation.

---

## ğŸ”¸ 5.3 Ã‰tape 2 â€” Pooling (Sous-Ã©chantillonnage)

### ğŸ¯ Objectif

RÃ©duire la taille des **feature maps** tout en gardant lâ€™information essentielle.
â†’ Moins de paramÃ¨tres, moins de calculs, moins dâ€™overfitting.

### ğŸ”§ Types de pooling :

* **Max pooling** : garde la valeur maximale du patch.
* **Average pooling** : moyenne du patch.
* **Global pooling** : rÃ©duit chaque map en une seule valeur.

### âš™ï¸ ParamÃ¨tres :

* **Taille du kernel** : souvent 2Ã—2.
* **Stride** : pas de dÃ©placement (souvent 2).

ğŸ“˜ Exemple :
Une feature map 4Ã—4 devient une 2Ã—2 aprÃ¨s max pooling 2Ã—2.


![Max Pooling](./images/Max_pooling.png)

---

## ğŸ”¸ 5.4 Ã‰tape 3 â€” Flattening

---

### ğŸ§© 5.4.1 Contexte : pourquoi le Flattening existe

Dans un **rÃ©seau de neurones convolutionnel (CNN)**, les premiÃ¨res couches (Convolution + Pooling) ont pour rÃ´le :

* dâ€™extraire des **caractÃ©ristiques locales** (bords, textures, formes, couleurs, etc.),
* et de produire des **feature maps** sous forme de **matrices 2D** (ou 3D si on compte les canaux).

Exemple :
AprÃ¨s plusieurs couches, on peut obtenir un tenseur de taille **(32, 32, 64)** :

* 32Ã—32 = dimensions spatiales (hauteur, largeur),
* 64 = nombre de **filtres** â†’ donc 64 **feature maps**.

ğŸ’¡ Chaque feature map correspond Ã  une caractÃ©ristique apprise (par ex. â€œbord verticalâ€, â€œcourbeâ€, â€œyeuxâ€, etc.).

ProblÃ¨me :
â¡ï¸ La prochaine Ã©tape du CNN est un **rÃ©seau de neurones dense (Fully Connected)**, or celui-ci **ne peut traiter que des vecteurs 1D** (liste de valeurs).

Câ€™est lÃ  quâ€™intervient le **Flattening**.

---

### ğŸ§® 5.4.2 DÃ©finition du Flattening

Le **Flattening** est une opÃ©ration de **mise Ã  plat** (ou â€œvectorisationâ€) :

> On transforme le tenseur 3D issu des convolutions/poolings en un **vecteur 1D**.


![Flattening](./images/Flattening.png)

Formellement :
[
\text{Flattening : } \mathbb{R}^{h \times w \times c} \longrightarrow \mathbb{R}^{(h \cdot w \cdot c)}
]

Exemple :

* Avant flattening : feature maps = (7, 7, 64)
* AprÃ¨s flattening : vecteur = (7Ã—7Ã—64) = 3136 valeurs.

---

### ğŸ§  5.4.3 InterprÃ©tation conceptuelle

On peut voir le flattening comme :

> â€œPrendre toutes les caractÃ©ristiques locales extraites dans les feature maps et les disposer en une seule ligne, pour les donner Ã  un classificateur.â€

Chaque valeur du vecteur final reprÃ©sente une **intensitÃ© dâ€™activation** dâ€™une caractÃ©ristique dans une rÃ©gion spÃ©cifique de lâ€™image.

ğŸ§© **Analogie** :
Câ€™est comme si, aprÃ¨s avoir dÃ©tectÃ© diffÃ©rents morceaux dâ€™un visage (yeux, nez, bouche), tu rangeais toutes ces informations sur une seule ligne avant de dÃ©cider si câ€™est un **chien**, un **chat**, ou un **humain**.

---

### âš™ï¸ 5.4.4 Exemple concret

Imaginons une **image 32Ã—32Ã—3 (RGB)**.
AprÃ¨s convolution et pooling, on obtient :

```
Feature maps = (8, 8, 32)
```

â¡ï¸ Le flattening transforme ce tenseur 3D en un **vecteur 1D de longueur 8Ã—8Ã—32 = 2048**.

Ce vecteur devient alors lâ€™**entrÃ©e de la premiÃ¨re couche fully connected (Dense)**, qui prendra ces 2048 valeurs comme **features globales** de lâ€™image.

---

### ğŸ’¡ 5.4.5 Pourquoi câ€™est utile

âœ… Il permet de connecter la partie **convolutionnelle** (apprentissage spatial) Ã  la partie **dense** (apprentissage dÃ©cisionnel).
âœ… Il conserve **toutes les informations extraites** jusque-lÃ .
âœ… Il rend le rÃ©seau **compatible avec les couches de type Dense**.


![Flattening Structure](./images/Flattening_structure.png)

---

### âš ï¸ 5.4.6 Points Ã  retenir

| Ã‰tape      | EntrÃ©e                    | Sortie     | RÃ´le                                               |
| ---------- | ------------------------- | ---------- | -------------------------------------------------- |
| Flattening | Tenseur 3D (Feature Maps) | Vecteur 1D | Convertir les features en entrÃ©e dâ€™un rÃ©seau dense |

---

### ğŸ§± 5.4.7 Exemple en code (Keras)

```python
from tensorflow.keras.layers import Flatten

# Exemple aprÃ¨s une convolution et un pooling
model.add(Conv2D(64, (3,3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2,2)))

# Flatten : passage de 3D Ã  1D
model.add(Flatten())

# Puis couches fully connected
model.add(Dense(128, activation='relu'))
model.add(Dense(10, activation='softmax'))
```

---

### ğŸ§® 5.4.8 InterprÃ©tation mathÃ©matique (facultative mais utile)

Le flattening est en rÃ©alitÃ© une **transformation linÃ©aire** :
[
x' = \text{vec}(X)
]
oÃ¹ `vec` (vectorisation) empile les colonnes dâ€™une matrice ou les valeurs dâ€™un tenseur dans un seul vecteur.

Cela ne change **aucune valeur**, seulement leur **forme**.
Les poids du rÃ©seau dense qui suivent vont alors apprendre Ã  pondÃ©rer chaque â€œfeature activÃ©eâ€ individuellement.

---

### ğŸ§© 5.4.9 MÃ©taphore visuelle

Imagine un **livre** :

* Les couches convolutionnelles et pooling reprÃ©sentent les **pages** (chaque page = un filtre qui dÃ©tecte un motif).
* Le flattening consiste Ã  **arracher toutes les pages, les dÃ©couper en petits morceaux, et les mettre Ã  plat sur une table**.
* Le rÃ©seau dense (fully connected) vient ensuite **analyser cette table complÃ¨te** pour tirer une conclusion finale.

---

## ğŸ”¸ 5.5 Ã‰tape 4 â€” Fully Connected Layer (FC)

Ces couches se comportent comme des **rÃ©seaux ANN classiques**.
Elles combinent toutes les features extraites pour prÃ©dire la classe finale.

![Fully Connected](./images/Fully_connected1.png)

ğŸ“˜ Exemple :

* Feature : â€œyeuxâ€, â€œnezâ€, â€œoreillesâ€
* Classe : â€œchienâ€, â€œchatâ€, â€œlapinâ€

Le CNN apprend **quelles combinaisons de features** correspondent Ã  quelle classe.

![Fully Connected Prediction](./images/FC_prediction.png)

---

## ğŸ”¸ 5.6 Ã‰tape 5 â€” Couche de sortie

* Applique une **fonction Softmax** pour obtenir les **probabilitÃ©s** de chaque classe :
  [
  P(y_i) = \frac{e^{z_i}}{\sum_j e^{z_j}}
  ]

* La **classe prÃ©dite** est celle avec la probabilitÃ© la plus Ã©levÃ©e.

---

## ğŸ”¸ 5.7 Ã‰tape 6 â€” Fonction de perte (Loss Function)

On utilise gÃ©nÃ©ralement la **Cross-Entropy** pour mesurer la performance du modÃ¨le :

[
L = -\sum y_i \log(\hat{y_i})
]

* Si la prÃ©diction sâ€™Ã©loigne de la vÃ©ritÃ© â†’ perte augmente.
* Objectif : **minimiser la perte** pour optimiser le rÃ©seau.

---

## ğŸ”¹ 6. RÃ©sumÃ© du pipeline CNN

| Ã‰tape | Nom de la couche        | Fonction principale           |
| ----- | ----------------------- | ----------------------------- |
| 1     | Convolution             | Extraire les features locales |
| 1â€™    | ReLU                    | Introduire la non-linÃ©aritÃ©   |
| 2     | Pooling                 | RÃ©duire la dimension spatiale |
| 3     | Flattening              | Convertir en vecteur 1D       |
| 4     | Fully Connected         | Combiner les features         |
| 5     | Softmax + Cross-Entropy | PrÃ©dire et Ã©valuer la classe  |

![CNN Process](./images/CNN_Process.png)

---

## ğŸ”¹ 7. Exemple en Keras

```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense

# CrÃ©ation du modÃ¨le CNN
model = Sequential([
    Conv2D(32, (3,3), activation='relu', input_shape=(64, 64, 3)),
    MaxPooling2D(pool_size=(2,2)),
    Conv2D(64, (3,3), activation='relu'),
    MaxPooling2D(pool_size=(2,2)),
    Flatten(),
    Dense(128, activation='relu'),
    Dense(1, activation='sigmoid')
])

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model.fit(X_train, y_train, epochs=25, batch_size=32, validation_split=0.2)
```

---

## ğŸ”¹ 8. Points forts et limites

âœ… **Avantages :**

* RÃ©duction du besoin dâ€™extraction manuelle de features.
* Excellente performance en vision par ordinateur.
* Poids partagÃ©s â†’ moins de paramÃ¨tres Ã  apprendre.

âš ï¸ **Limites :**

* NÃ©cessite beaucoup de donnÃ©es dâ€™entraÃ®nement.
* TrÃ¨s coÃ»teux en calcul (GPU recommandÃ©).
* Moins adaptÃ© aux donnÃ©es tabulaires classiques.

---

## ğŸ”¹ 9. Applications rÃ©elles

* ğŸ–¼ï¸ Reconnaissance faciale
* ğŸš— Voitures autonomes (dÃ©tection de piÃ©tons, feux, routes)
* ğŸ©º Imagerie mÃ©dicale (IRM, radiographie)
* ğŸ“± Reconnaissance dâ€™objets sur smartphone
* ğŸ“· Classification dâ€™Ã©motions sur visages