# ğŸ§  Deep Learning â€“ Recurrent Neural Networks (RNN)

---

## ğŸ“š RÃ©fÃ©rences principales

* *Deep Learning Aâ€“Z* â€” Kirill Eremenko & Hadelin de Ponteves (@SuperDataScience)
* *Deep Learning â€“ Artificial Intelligence* â€” Sarah Malaeb

---

## ğŸ”¹ 1. Introduction

Les **RÃ©seaux de Neurones RÃ©currents (RNN)** sont une architecture de Deep Learning conÃ§ue pour traiter des **donnÃ©es sÃ©quentielles ou temporelles**.

ğŸ§  Contrairement Ã  un **rÃ©seau de neurones classique (ANN)**, le RNN possÃ¨de une **mÃ©moire interne** qui lui permet de **retenir lâ€™information prÃ©cÃ©dente** pour influencer la sortie suivante.

Exemples de donnÃ©es sÃ©quentielles :

* ğŸ“ du texte (phrases, documents),
* ğŸµ des signaux audio ou musicaux,
* ğŸ“ˆ des sÃ©ries temporelles (financiÃ¨res, mÃ©tÃ©o, IoT).

---

## ğŸ”¹ 2. Motivation : pourquoi un RNN ?

Les rÃ©seaux **feed-forward (ANN)** traitent chaque donnÃ©e indÃ©pendamment.
ğŸ‘‰ Ils ne savent pas que la donnÃ©e prÃ©cÃ©dente influence la suivante.

Exemple :
Si tu veux prÃ©dire le mot suivant dans

> â€œLe chat mange une â€¦â€
> un rÃ©seau classique **ne se souvient pas** des mots prÃ©cÃ©dents.

ğŸ§© Les **RNN**, eux, gardent une trace de ce quâ€™ils ont vu avant :

> â€œLe chat mange une ğŸŸâ€
> â¡ï¸ Le modÃ¨le sait que â€œmangeâ€ prÃ©cÃ¨de â€œuneâ€ et que â€œuneâ€ est souvent suivi dâ€™un objet comestible.

![ANN vs RNN](./Image/ANN_vs_RNN.png)

---

## ğŸ”¹ 3. DÃ©finition et principe

Un **RNN (Recurrent Neural Network)** est un rÃ©seau dans lequel **les connexions forment une boucle temporelle**.

Chaque neurone cachÃ© envoie **sa sortie prÃ©cÃ©dente** comme **entrÃ©e de la couche cachÃ©e suivante**, en plus de la donnÃ©e actuelle.

### ğŸ” SchÃ©ma conceptuel

```
xt  â†’  [ RNN Cell ] â†’ ht  â†’  yt
 â†‘          â†“
xt-1 â†’ [ RNN Cell ] â†’ ht-1
 â†‘          â†“
xt-2 â†’ [ RNN Cell ] â†’ ht-2
```

Formules :
[
h_t = f(W_x x_t + W_h h_{t-1} + b)
]
[
y_t = g(W_y h_t)
]

oÃ¹ :

* (x_t) : entrÃ©e Ã  lâ€™instant t
* (h_t) : Ã©tat cachÃ© (mÃ©moire du rÃ©seau)
* (y_t) : sortie du rÃ©seau
* (W_x, W_h, W_y) : poids appris
* (f, g) : fonctions dâ€™activation

![RNN](./Image/RNN.png)

---

## ğŸ”¹ 4. Exemple intuitif

Imagine que tu lis une phrase mot par mot :

> â€œLe chat saute sur la tableâ€

Un RNN :

1. Lit â€œLeâ€ â†’ crÃ©e une premiÃ¨re mÃ©moire (h_1)
2. Lit â€œchatâ€ â†’ se souvient de â€œLeâ€
3. Lit â€œsauteâ€ â†’ sait quâ€™un sujet â€œLe chatâ€ prÃ©cÃ¨de
4. Lit â€œtableâ€ â†’ comprend la structure complÃ¨te

â¡ï¸ Il comprend donc le **contexte**.

---

## ğŸ”¹ 5. DiffÃ©rence entre CNN et RNN

| CritÃ¨re         | CNN                               | RNN                                     |
| --------------- | --------------------------------- | --------------------------------------- |
| Type de donnÃ©es | Images, matrices                  | SÃ©quences (texte, audio, sÃ©ries)        |
| Connexions      | Spatiales (voisinage)             | Temporelles (temps t-1 â†’ t)             |
| MÃ©moire         | Non                               | Oui                                     |
| Applications    | Vision, dÃ©tection, reconnaissance | Langage, sÃ©ries temporelles, prÃ©diction |

---

## ğŸ”¹ 6. Types de structures RNN

| Type             | Description                           | Exemple                                  |
| ---------------- | ------------------------------------- | ---------------------------------------- |
| **One-to-One**   | EntrÃ©e unique â†’ Sortie unique         | Classification simple                    |
| **One-to-Many**  | EntrÃ©e unique â†’ Plusieurs sorties     | GÃ©nÃ©ration dâ€™image en texte (captioning) |
| **Many-to-One**  | Plusieurs entrÃ©es â†’ Une sortie        | Analyse de sentiment                     |
| **Many-to-Many** | Plusieurs entrÃ©es â†’ Plusieurs sorties | Traduction automatique, sous-titrage     |

![One to Many](./Image/O2M.png)

![Many to One](./Image/M2O.png)

![Many to Many](./Image/M2M.png)

---

## ğŸ”¹ 7. ProblÃ¨me du Vanishing Gradient

### ğŸ§© 7.1. Le problÃ¨me

Lors de la backpropagation Ã  travers le temps (**BPTT**), les gradients sont multipliÃ©s plusieurs fois par les mÃªmes poids rÃ©currents (w_{rec}).

[
\text{Si } w_{rec} < 1, \text{ alors le gradient devient de plus en plus petit : } \rightarrow 0
]

â¡ï¸ Les poids ne se mettent plus Ã  jour efficacement â†’ le rÃ©seau **oublie les informations anciennes**.

Inversement, si (w_{rec} > 1), on a un **exploding gradient** (valeurs trop grandes).

![Vanishing Gradient Problem](./Image/Vanishing_Gradient_Problem.png)

---

### âš ï¸ 7.2. ConsÃ©quences

* Le rÃ©seau ne â€œmÃ©moriseâ€ que les informations rÃ©centes.
* Lâ€™apprentissage devient instable ou lent.

---

### ğŸ”§ 7.3. Solutions

#### âœ… Pour le **vanishing gradient**

* Initialisation adaptÃ©e des poids (Xavier, He).
* RÃ©seaux Ã  mÃ©moire longue : **LSTM** et **GRU**.
* Troncature de la rÃ©tropropagation (Backpropagation Through Time limitÃ©e).

#### âœ… Pour le **exploding gradient**

* Clipping du gradient (on fixe une valeur maximale).

---

## ğŸ”¹ 8. Les LSTM (Long Short-Term Memory)

### ğŸ’¡ Motivation

Le LSTM est une **Ã©volution du RNN** conÃ§ue pour **retenir les informations sur de longues pÃ©riodes**.

---

## ğŸ”¸ 8.1. Architecture interne dâ€™un LSTM (Long Short-Term Memory)

### ğŸ”¹ 8.1.1 Structure gÃ©nÃ©rale

Un **LSTM** est une cellule de RNN amÃ©liorÃ©e, capable de **conserver ou dâ€™oublier des informations** Ã  volontÃ©, grÃ¢ce Ã  un mÃ©canisme de **portes (gates)**.

Chaque cellule LSTM reÃ§oit :

* lâ€™entrÃ©e actuelle ( x_t ),
* la sortie prÃ©cÃ©dente ( h_{t-1} ),
* et la mÃ©moire prÃ©cÃ©dente ( C_{t-1} ).

Elle renvoie :

* une **nouvelle mÃ©moire** ( C_t ),
* une **nouvelle sortie** ( h_t ).

ğŸ“˜ En rÃ©sumÃ© :
[
(x_t, h_{t-1}, C_{t-1}) \longrightarrow (h_t, C_t)
]

---

### ğŸ”¹ 8.1.2 Les 3 portes dâ€™un LSTM

Chaque â€œporteâ€ est un petit rÃ©seau qui dÃ©cide **combien dâ€™information** doit passer.

| Porte              | RÃ´le                       | Fonction dâ€™activation             |
| ------------------ | -------------------------- | --------------------------------- |
| ğŸ§¹ **Forget Gate** | Oublier les infos inutiles | Sigmoid (0 â†’ oublier, 1 â†’ garder) |
| ğŸ’¾ **Input Gate**  | Ajouter de nouvelles infos | Sigmoid + tanh                    |
| ğŸšª **Output Gate** | Produire la sortie         | Sigmoid + tanh                    |

---

### ğŸ”¹ 8.1.3 La Forget Gate (porte dâ€™oubli)

Formule :
[
f_t = \sigma(W_f [h_{t-1}, x_t] + b_f)
]

ğŸ“˜ Elle reÃ§oit la concatÃ©nation de (h_{t-1}) et (x_t), puis dÃ©cide **quelles informations de la mÃ©moire passÃ©e (C_{t-1})** doivent Ãªtre oubliÃ©es.

ğŸ§  InterprÃ©tation :

* Si (f_t â‰ˆ 0) â†’ on oublie cette information.
* Si (f_t â‰ˆ 1) â†’ on la garde.

ğŸ’¬ Exemple :

> Si tu lis une phrase : â€œLe chat est mignon, mais il **griffe**.â€
> La forget gate va dÃ©cider dâ€™oublier â€œmignonâ€ (Ã©motion positive) pour retenir â€œgriffeâ€ (information nÃ©gative).

---

### ğŸ”¹ 8.1.4 La Input Gate (porte dâ€™entrÃ©e)

Elle dÃ©cide **quelles nouvelles informations** ajouter Ã  la mÃ©moire Ã  long terme.

Formules :
[
i_t = \sigma(W_i [h_{t-1}, x_t] + b_i)
]
[
\tilde{C}*t = \tanh(W_C [h*{t-1}, x_t] + b_C)
]

Ensuite, la nouvelle mÃ©moire candidate (\tilde{C}*t) est **pondÃ©rÃ©e** par (i_t) :
[
C_t = f_t * C*{t-1} + i_t * \tilde{C}_t
]

ğŸ§  InterprÃ©tation :

* (i_t) â†’ dÃ©cide *si on met Ã  jour* une information.
* (\tilde{C}_t) â†’ reprÃ©sente *la nouvelle information Ã  stocker*.

ğŸ’¬ Exemple :

> Dans une phrase, â€œle chien **aboie** fortâ€, le LSTM ajoute Ã  sa mÃ©moire lâ€™idÃ©e dâ€™â€œaboiementâ€.

---

### ğŸ”¹ 8.1.5 La Output Gate (porte de sortie)

Elle contrÃ´le **ce que la cellule renvoie Ã  lâ€™extÃ©rieur** (mÃ©moire courte (h_t)).

Formules :
[
o_t = \sigma(W_o [h_{t-1}, x_t] + b_o)
]
[
h_t = o_t * \tanh(C_t)
]

ğŸ§  InterprÃ©tation :

* (o_t) â†’ filtre la mÃ©moire interne pour produire la sortie visible.
* (\tanh(C_t)) â†’ ramÃ¨ne la mÃ©moire Ã  une Ã©chelle comprise entre -1 et 1.

ğŸ’¬ Exemple :

> Si le LSTM a lu toute la phrase â€œLe chat dort paisiblementâ€,
> la output gate dÃ©cide que la sortie finale (h_t) sera centrÃ©e sur â€œdortâ€ = action principale.

---

### ğŸ”¹ 8.1.6 SynthÃ¨se des formules du LSTM

[
\begin{cases}
f_t = \sigma(W_f [h_{t-1}, x_t] + b_f) & \text{Forget Gate}\
i_t = \sigma(W_i [h_{t-1}, x_t] + b_i) & \text{Input Gate}\
\tilde{C}*t = \tanh(W_C [h*{t-1}, x_t] + b_C) & \text{MÃ©moire candidate}\
C_t = f_t * C_{t-1} + i_t * \tilde{C}*t & \text{Nouvelle mÃ©moire}\
o_t = \sigma(W_o [h*{t-1}, x_t] + b_o) & \text{Output Gate}\
h_t = o_t * \tanh(C_t) & \text{Nouvelle sortie}
\end{cases}
]

---

### ğŸ”¹ 8.1.7 InterprÃ©tation complÃ¨te du flux dâ€™information

1. ğŸ”¹ **EntrÃ©e (x_t)** et **mÃ©moire prÃ©cÃ©dente (h_{t-1}, C_{t-1})** arrivent dans la cellule.
2. ğŸ§¹ La **forget gate** dÃ©cide ce quâ€™on oublie.
3. ğŸ’¾ La **input gate** dÃ©cide ce quâ€™on ajoute.
4. ğŸ§® On **met Ã  jour la mÃ©moire (C_t)**.
5. ğŸšª La **output gate** dÃ©cide quelle partie de la mÃ©moire devient la sortie (h_t).

---

### ğŸ”¹ 8.1.8 Visualisation

```markdown
![Architecture LSTM](https://upload.wikimedia.org/wikipedia/commons/3/3b/The_LSTM_cell.png)
```

ğŸ§  Le schÃ©ma montre bien :

* Les 3 portes (Forget, Input, Output)
* Le flux de la mÃ©moire (C_t) en haut
* Le flux des sorties (h_t) en bas
* Les opÃ©rateurs (\sigma) et (\tanh)

---

### ğŸ”¹ 8.1.9 Points clÃ©s Ã  retenir

âœ… LSTM â‰  simple RNN :

> il choisit intelligemment quoi **oublier**, **retenir**, et **transmettre**.

âœ… Les LSTM permettent :

* De **mÃ©moriser des dÃ©pendances longues** dans les sÃ©quences (ex : contexte grammatical, relations de mots lointains).
* Dâ€™Ã©viter le **vanishing gradient**.

âœ… UtilisÃ© massivement dans :

* Traduction automatique (Seq2Seq)
* Analyse de sentiments
* GÃ©nÃ©ration de texte (ChatGPT, entre autres ğŸ˜„)

---

## ğŸ”¹ 9. GRU (Gated Recurrent Unit)

Une version plus simple du LSTM, avec seulement **2 portes** :

* ğŸ” **Update Gate** (mÃ©lange input + forget)
* ğŸ”’ **Reset Gate**

Moins de paramÃ¨tres â†’ plus rapide Ã  entraÃ®ner.

---

## ğŸ”¹ 10. Application : NLP (Natural Language Processing)

Les RNN et LSTM sont trÃ¨s utilisÃ©s pour le **traitement du langage naturel (NLP)** :

* PrÃ©diction de mots (Word Prediction)
* Traduction automatique (Seq2Seq)
* Analyse de sentiment (Positive / Negative)
* GÃ©nÃ©ration de texte (Chatbots, RÃ©sumÃ©s automatiques)

---

### âš™ï¸ Ã‰tapes du traitement de texte (NLP)

1. **Tokenization** â†’ dÃ©coupe du texte en mots ou phrases.
2. **Stop-word removal** â†’ suppression des mots frÃ©quents inutiles (â€œleâ€, â€œdeâ€, â€œetâ€...).
3. **Lemmatisation / Stemming** â†’ rÃ©duction des mots Ã  leur racine (â€œmangeaitâ€ â†’ â€œmangerâ€).
4. **Part-of-speech tagging** â†’ identification grammaticale (verbe, nom, adjectif).
5. **Word Embedding** â†’ transformation des mots en vecteurs numÃ©riques.

---

### ğŸ“Š Techniques de Word Embedding

| MÃ©thode          | Description                              |
| ---------------- | ---------------------------------------- |
| **Bag of Words** | Compte la frÃ©quence des mots             |
| **TF-IDF**       | PondÃ¨re selon lâ€™importance du mot        |
| **Word2Vec**     | Apprend des vecteurs sÃ©mantiques         |
| **FastText**     | Prend en compte la morphologie           |
| **Seq2Seq**      | Architecture pour traduction automatique |

---

## ğŸ”¹ 11. Exemple pratique : PrÃ©diction de texte avec LSTM (Keras)

```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Embedding

model = Sequential([
    Embedding(input_dim=10000, output_dim=128, input_length=50),
    LSTM(128, return_sequences=False),
    Dense(64, activation='relu'),
    Dense(1, activation='sigmoid')
])

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
model.summary()
```

---

## ğŸ”¹ 12. RÃ©capitulatif

| RÃ©seau         | SpÃ©cificitÃ©                   | Utilisation                 |
| -------------- | ----------------------------- | --------------------------- |
| **ANN**        | DonnÃ©es indÃ©pendantes         | Classification simple       |
| **CNN**        | DonnÃ©es spatiales (images)    | Vision par ordinateur       |
| **RNN**        | DonnÃ©es sÃ©quentielles (temps) | Texte, sÃ©ries, sons         |
| **LSTM / GRU** | MÃ©moire longue durÃ©e          | NLP, prÃ©vision sÃ©quentielle |

---

## âœ… Ã€ retenir

ğŸ”¹ Les RNN apprennent Ã  partir de **sÃ©quences dÃ©pendantes du temps**.
ğŸ”¹ Ils utilisent leur **Ã©tat cachÃ©** pour mÃ©moriser des informations prÃ©cÃ©dentes.
ğŸ”¹ Les **LSTM** et **GRU** corrigent les limites des RNN classiques (vanishing gradient).
ğŸ”¹ Ils sont Ã  la base des modÃ¨les modernes de traitement du langage (NLP, ChatGPT, etc.).

