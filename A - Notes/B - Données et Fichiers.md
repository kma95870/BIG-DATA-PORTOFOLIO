# ğŸ“‚ LeÃ§on : Les Fichiers dans le Monde de la DonnÃ©e

## ğŸ—­ Introduction

Dans lâ€™Ã©cosystÃ¨me de la data, les fichiers sont les **conteneurs de l'information**. Que ce soit dans des bases relationnelles, des APIs, des Data Lakes ou des flux temps rÃ©el, **tout commence et finit souvent par un fichier**.

Comprendre les types de fichiers, leur structure, leur usage et leurs limites est **essentiel** pour toute personne travaillant dans lâ€™ingÃ©nierie ou lâ€™analyse de donnÃ©es.

---

## ğŸ§± 1. CatÃ©gories de fichiers dans la data

| **CatÃ©gorie**               | **Types de fichiers**                    | **Utilisation typique**                           |
| --------------------------- | ---------------------------------------- | ------------------------------------------------- |
| Fichiers plats              | `.csv`, `.tsv`, `.txt`                   | Exports Excel, logs, donnÃ©es simples              |
| Fichiers structurÃ©s texte   | `.json`, `.xml`, `.yaml`                 | API, fichiers de config, donnÃ©es semi-structurÃ©es |
| Fichiers binaires           | `.parquet`, `.avro`, `.orc`              | Big Data, Data Lake, Spark                        |
| Feuilles de calcul          | `.xls`, `.xlsx`, `.ods`                  | Reporting, analyses mÃ©tier                        |
| Fichiers de base de donnÃ©es | `.sqlite`, `.db`, `.mdb`, `.rds`, `.dta` | Stockage local ou format d'export de BDD          |
| Fichiers compressÃ©s         | `.zip`, `.gz`, `.bz2`, `.tar`            | RÃ©duction taille de stockage ou transfert         |
| Fichiers modÃ¨les ML         | `.pkl`, `.joblib`, `.onnx`, `.h5`, `.pb` | Sauvegarde de modÃ¨les entraÃ®nÃ©s                   |
| Fichiers logs & traces      | `.log`, `.txt`, `.har`, `.out`           | Monitoring, debugging                             |

---

## ğŸš€ 2. Focus sur les fichiers Big Data

Les formats **Big Data** sont conÃ§us pour traiter de **gros volumes** de donnÃ©es de maniÃ¨re **rapide, efficace et distribuÃ©e**. Les plus courants sont :

* **CSV**
* **JSON**
* **Parquet**
* **Avro**
* **ORC**

---

### ğŸ“ 2.1 CSV (Comma-Separated Values)

#### âœ… DÃ©finition

Format **texte tabulaire**. Chaque ligne est un enregistrement, chaque colonne est sÃ©parÃ©e par une virgule, un point-virgule ou une tabulation.

#### ğŸ” Exemple

```csv
id,name,age
1,Alice,30
2,Bob,25
```

#### ğŸ§  Contexte dâ€™usage

* Fichiers dâ€™export (Excel, bases de donnÃ©es)
* DonnÃ©es simples, non hiÃ©rarchiques

#### ğŸ› ï¸ Manipulation

**Python (pandas)**

```python
import pandas as pd
df = pd.read_csv("data.csv")
df.to_csv("out.csv", index=False)
```

**Spark**

```python
df = spark.read.csv("data.csv", header=True, inferSchema=True)
df.write.csv("output/", header=True)
```

#### âš ï¸ Limites

* Pas de schÃ©ma.
* DonnÃ©es typÃ©es manuellement.
* Inefficace pour la lecture sÃ©lective de colonnes.

---

### ğŸ—ï¸ 2.2 JSON (JavaScript Object Notation)

#### âœ… DÃ©finition

Format **texte hiÃ©rarchique**, basÃ© sur des paires clÃ©-valeur. Permet des structures **imbriquÃ©es**.

#### ğŸ” Exemple

```json
[
  {"id": 1, "user": {"name": "Alice", "age": 30}},
  {"id": 2, "user": {"name": "Bob", "age": 25}}
]
```

#### ğŸ§  Contexte dâ€™usage

* API REST
* NoSQL (MongoDB)
* Ã‰change de donnÃ©es entre systÃ¨mes

#### ğŸ› ï¸ Manipulation

**Python**

```python
import json
with open("file.json") as f:
    data = json.load(f)
```

**Spark**

```python
df = spark.read.json("file.json")
df.write.json("output/")
```

#### âš ï¸ Limites

* Volumineux (verbeux).
* Structure non tabulaire â†’ traitement plus complexe.

---

### ğŸ•¦ 2.3 Parquet

#### âœ… DÃ©finition

Format **binaire colonnaire**, compressÃ©, avec schÃ©ma intÃ©grÃ©. TrÃ¨s utilisÃ© dans lâ€™Ã©cosystÃ¨me Hadoop/Spark.

#### ğŸ” Illustration interne (logique)

```
[Colonne1] [Colonne2] [Colonne3]
â†’ Lecture optimisÃ©e uniquement sur les colonnes nÃ©cessaires
```

#### ğŸ§  Contexte dâ€™usage

* Data Lake
* Spark, Hive, Presto
* Stockage optimisÃ© Cloud (S3, GCS)

#### ğŸ› ï¸ Manipulation

**Pandas (nÃ©cessite `pyarrow` ou `fastparquet`)**

```python
df = pd.read_parquet("data.parquet")
df.to_parquet("out.parquet")
```

**Spark**

```python
df = spark.read.parquet("data.parquet")
df.write.parquet("output/")
```

#### âš ï¸ Limites

* Illisible sans outils.
* Plus lent en Ã©criture (surtout petits fichiers).

---

### ğŸ”´ 2.4 Avro

#### âœ… DÃ©finition

Format **binaire orientÃ© ligne** avec **schÃ©ma JSON intÃ©grÃ©**. IdÃ©al pour les systÃ¨mes distribuÃ©s (Kafka).

#### ğŸ” Exemple de schÃ©ma

```json
{
  "type": "record",
  "name": "User",
  "fields": [
    {"name": "id", "type": "int"},
    {"name": "name", "type": "string"}
  ]
}
```

#### ğŸ§  Contexte dâ€™usage

* Kafka
* Streaming temps rÃ©el
* Ã‰change de donnÃ©es Ã©volutif

#### ğŸ› ï¸ Manipulation

**PySpark**

```python
df = spark.read.format("avro").load("data.avro")
df.write.format("avro").save("output/")
```

**Python avec `fastavro`**

```python
from fastavro import reader
with open("file.avro", "rb") as f:
    for record in reader(f):
        print(record)
```

#### âš ï¸ Limites

* Moins performant pour requÃªtes analytiques (vs Parquet).
* Moins lisible.

---

### ğŸ•˜ 2.5 ORC (Optimized Row Columnar)

#### âœ… DÃ©finition

Format **colonnaire binaire** optimisÃ© pour Hadoop et Hive, avec compression trÃ¨s efficace.

#### ğŸ§  Contexte dâ€™usage

* Hadoop / Hive
* Data warehouse open source
* RequÃªtes analytiques lourdes

#### ğŸ› ï¸ Manipulation

**Spark**

```python
df = spark.read.orc("data.orc")
df.write.orc("output/")
```

#### âš ï¸ Limites

* Moins universel (moins supportÃ© hors Hadoop).
* Moins rÃ©pandu que Parquet en cloud.

---

## â™»ï¸ Tableau comparatif des formats Big Data

| Format      | Type    | Compression | Lecture colonne | Streaming | SchÃ©ma intÃ©grÃ© | Cas typique              |
| ----------- | ------- | ----------- | --------------- | --------- | -------------- | ------------------------ |
| **CSV**     | Texte   | âŒ           | âŒ               | âŒ         | âŒ              | Fichiers dâ€™export / logs |
| **JSON**    | Texte   | âŒ           | âŒ               | âœ… (JSONL) | âŒ              | API / NoSQL              |
| **Parquet** | Binaire | âœ… (Snappy)  | âœ…               | âŒ         | âœ…              | Data Lake / S3 / GCP     |
| **Avro**    | Binaire | âœ…           | Moyen           | âœ…         | âœ… (JSON)       | Kafka / Streaming        |
| **ORC**     | Binaire | âœ…âœ…          | âœ…+              | âŒ         | âœ…              | Hive / Hadoop            |

---

## âœ… Conclusion

MaÃ®triser les formats de fichiers, câ€™est :

* **choisir le bon outil** pour le bon besoin.
* **optimiser le stockage et les performances**.
* **faciliter lâ€™automatisation des pipelines** dans les architectures modernes (ETL, Data Lake, Kafkaâ€¦).

---
