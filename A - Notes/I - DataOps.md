# ğŸ“š DevOps pour la Data (DataOps)

---

## 1. ğŸŒ Introduction : Quâ€™est-ce que le DevOps pour la Data ?

* **DevOps classique** = rapprocher Dev (dÃ©veloppeurs) et Ops (ops/infra) â†’ livraisons plus rapides, plus fiables.
* **DataOps** = adaptation de DevOps au cycle de vie **des donnÃ©es** :

  * AccÃ©lÃ©rer ingestion â†’ transformation â†’ livraison.
  * Automatiser la qualitÃ© et les dÃ©ploiements des pipelines.
  * AmÃ©liorer la collaboration entre **Data Engineers, Analysts, Scientists et Ops**.

ğŸ‘‰ Objectif : **livrer de la valeur Data plus vite, avec plus de qualitÃ© et moins dâ€™erreurs**.

---

## 2. ğŸ”„ Cycle de vie DataOps

1. **Ingestion** â†’ Collecte des donnÃ©es (batch, streaming).
2. **Stockage** â†’ Data Lake (S3, HDFS) + Data Warehouse (BigQuery, Snowflake, Redshift).
3. **Transformation** â†’ ETL/ELT (Spark, dbt, Airflow, Dataiku).
4. **QualitÃ©** â†’ tests, validation de schÃ©mas (Great Expectations).
5. **DÃ©ploiement** â†’ CI/CD pour pipelines, modÃ¨les ML, jobs Spark.
6. **Orchestration** â†’ Airflow, Prefect, Dagster.
7. **Monitoring** â†’ performance + qualitÃ© + fraisheur des donnÃ©es.
8. **Consommation** â†’ BI (Tableau, Power BI), APIs, modÃ¨les ML.

---

## 3. ğŸ§° Les briques DevOps appliquÃ©es Ã  la Data

| Domaine                    | Outils principaux                  | Cas dâ€™usage Data                                |
| -------------------------- | ---------------------------------- | ----------------------------------------------- |
| **Versioning**             | Git, DVC, LakeFS                   | Versionner code ET donnÃ©es                      |
| **CI/CD**                  | GitHub Actions, GitLab CI, Jenkins | DÃ©ploiement auto de pipelines, tests de donnÃ©es |
| **Containerisation**       | Docker                             | Standardiser un ETL, un modÃ¨le ML               |
| **Orchestration**          | Airflow, Prefect, Dagster          | GÃ©rer dÃ©pendances de jobs                       |
| **Infrastructure as Code** | Terraform, Ansible                 | CrÃ©er clusters Spark, bases, S3                 |
| **Cloud & Stockage**       | AWS S3, GCP, Azure                 | Data Lake, DWH                                  |
| **Monitoring**             | Grafana, Prometheus, ELK           | ObservabilitÃ©, alertes                          |
| **QualitÃ© des donnÃ©es**    | Great Expectations, dbt tests      | ContrÃ´les de cohÃ©rence et fraÃ®cheur             |
| **Secrets**                | Vault, KMS, Kubernetes Secrets     | SÃ©curiser accÃ¨s DB, API, S3                     |

---

## 4. ğŸ”„ CI/CD expliquÃ© simplement

### 1. ğŸ“Œ DÃ©finition

* **CI = Continuous Integration (IntÃ©gration Continue)**
  ğŸ‘‰ Automatiser le fait de **tester et valider** ton code/data **Ã  chaque modification**.

* **CD = Continuous Delivery / Continuous Deployment (Livraison & DÃ©ploiement Continu)**
  ğŸ‘‰ Automatiser le fait de **mettre en production** ton code/data de faÃ§on **rapide, fiable et rÃ©pÃ©table**.

---

### 2. âš™ï¸ SchÃ©ma du cycle CI/CD

```
DÃ©veloppeur â†’ Git (commit/push) 
      â†“
Pipeline CI (tests automatiques, qualitÃ© de code, tests de donnÃ©es)
      â†“
Pipeline CD (dÃ©ploiement auto â†’ ETL, DAG Airflow, modÃ¨le ML, API)
      â†“
Production (cluster Spark, Airflow, Data Lake, API BI)
```

---

### 3. ğŸ” CI (Continuous Integration)

But = **vÃ©rifier que le code est bon avant de lâ€™intÃ©grer**.

Exemples dans la Data :

* Tests unitaires sur du code Python ou PySpark.
* Tests de **qualitÃ© des donnÃ©es** (ex. pas de doublons, pas de valeurs manquantes inattendues).
* VÃ©rification que les scripts SQL sâ€™exÃ©cutent sans erreur.
* Build dâ€™images Docker reproductibles.

ğŸ‘‰ CI se lance **Ã  chaque `git push` ou `merge request`**.

---

### 4. ğŸš€ CD (Continuous Delivery/Deployment)

But = **mettre automatiquement en production ce qui a Ã©tÃ© validÃ©**.

Exemples dans la Data :

* DÃ©ployer un DAG Airflow mis Ã  jour dans le cluster.
* DÃ©ployer un job Spark packagÃ© en Docker sur Kubernetes.
* Mettre Ã  jour un modÃ¨le ML dans une API.
* RafraÃ®chir un dashboard (ex : dbt + Tableau).

ğŸ‘‰ CD se dÃ©clenche aprÃ¨s la CI si tout est vert âœ….

---

### 5. ğŸ”§ Exemple pratique (GitHub Actions)

### Cas : un ETL PySpark â†’ testÃ© puis dÃ©ployÃ© dans Airflow

```yaml
name: Data Pipeline CI/CD
on: [push]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - name: Install deps
        run: pip install -r requirements.txt
      - name: Run unit tests
        run: pytest tests/

  deploy:
    runs-on: ubuntu-latest
    needs: test
    steps:
      - uses: actions/checkout@v2
      - name: Deploy DAG
        run: ./deploy_to_airflow.sh
```

---

### 6. ğŸ¯ Avantages de CI/CD pour la Data

* **Moins dâ€™erreurs** : tout est testÃ© avant dâ€™arriver en prod.
* **Plus rapide** : pas besoin de dÃ©ployer Ã  la main.
* **Reproductible** : mÃªme pipeline de tests et de dÃ©ploiement pour tout le monde.
* **Collaboration** : plusieurs data engineers/scientists travaillent sans casser le projet.

---

âœ… En rÃ©sumÃ© :

* **CI = tester automatiquement le code + donnÃ©es** Ã  chaque changement.
* **CD = dÃ©ployer automatiquement en production si tout est OK**.

---

## 5. ğŸ“¦ CI/CD appliquÃ© Ã  la Data

### CI (Integration Continue) :

* VÃ©rifie le code (lint, tests unitaires).
* VÃ©rifie les **tests de donnÃ©es** (schÃ©ma, doublons, nullitÃ©).
* VÃ©rifie la reproductibilitÃ© (Docker build).

### CD (DÃ©ploiement Continu) :

* DÃ©ploie un DAG Airflow.
* DÃ©ploie un job Spark sur cluster (on-prem ou cloud).
* DÃ©ploie un modÃ¨le ML en API.

ğŸ‘‰ Exemple GitHub Actions pour ETL PySpark :

```yaml
name: CI/CD Data Pipeline
on: [push]
jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - name: Install deps
        run: pip install -r requirements.txt
      - name: Run tests
        run: pytest tests/
      - name: Deploy to Airflow
        run: ./deploy_dag.sh
```

---

## 6. ğŸ§ª Tests en DataOps

### a) **Tests unitaires de code**

* Exemple : fonction PySpark qui bucketise un Ã¢ge.
* Outils : `pytest`, `chispa` (comparaison DataFrames).

### b) **Tests de donnÃ©es**

* Great Expectations â†’ ex : "colonne age > 0".
* dbt tests â†’ unique, not null, foreign keys.

### c) **Tests dâ€™intÃ©gration**

* Testcontainers (Kafka, Postgres, MinIO).
* Moto (mock AWS).

### d) **Tests End-to-End**

* Pipeline complet : ingestion â†’ transformation â†’ stockage â†’ consommation.
* ValidÃ© en staging avant la prod.

---

## 7. ğŸ—ï¸ Infrastructure as Code (IaC)

* **Terraform** â†’ provisionne les ressources Cloud :

  * Buckets S3 / Data Lake.
  * Cluster Spark EMR ou GKE.
  * Base Redshift, BigQuery.

ğŸ‘‰ Exemple Terraform pour un bucket S3 :

```hcl
resource "aws_s3_bucket" "data_lake" {
  bucket = "my-data-lake"
  acl    = "private"
}
```

* **Ansible** â†’ configure les serveurs (install Spark, Airflow, Kafka).

---

## 8. â˜¸ï¸ Conteneurs & Orchestration

### Docker :

* Packager un script ETL (Python, PySpark).
* DÃ©ployer un modÃ¨le ML (FastAPI, Flask).

Exemple Dockerfile :

```dockerfile
FROM python:3.9
COPY requirements.txt .
RUN pip install -r requirements.txt
COPY . /app
WORKDIR /app
CMD ["python", "etl.py"]
```

### Orchestration (Airflow / Prefect / Dagster) :

* DAGs = dÃ©pendances entre jobs (ex : ingestion â†’ transformation â†’ load).
* Monitoring intÃ©grÃ©.
* Relance en cas dâ€™Ã©chec.

Exemple Airflow DAG :

```python
from airflow import DAG
from airflow.operators.bash import BashOperator
from datetime import datetime

with DAG("etl_pipeline", start_date=datetime(2023,1,1), schedule_interval="@daily") as dag:
    ingest = BashOperator(task_id="ingest", bash_command="python ingest.py")
    transform = BashOperator(task_id="transform", bash_command="python transform.py")
    load = BashOperator(task_id="load", bash_command="python load.py")

    ingest >> transform >> load
```

---

## 9. ğŸ“Š Monitoring en DataOps

* **ObservabilitÃ© technique** :

  * CPU, mÃ©moire (Prometheus + Grafana).
  * Logs (ELK Stack).
* **ObservabilitÃ© Data** :

  * FraÃ®cheur des tables (Airflow sensors).
  * QualitÃ© des donnÃ©es (Great Expectations).
  * Data Drift en ML (EvidentlyAI, MLflow).

ğŸ‘‰ Exemple : alerte Slack si une table nâ€™a pas Ã©tÃ© mise Ã  jour depuis 24h.

---

## 10. ğŸ” SÃ©curitÃ© et Gouvernance

* Secrets : stockÃ©s dans Vault / Secrets Manager.
* AccÃ¨s aux donnÃ©es : IAM, RBAC.
* RGPD : anonymisation, masquage, minimisation.
* Lineage : Data Catalog (ex : Collibra, DataHub, Amundsen).

---

## 11. ğŸš¦ Bonnes pratiques DataOps

1. **Versionner tout** (code + donnÃ©es + infra).
2. **Automatiser les tests** â†’ pipeline bloquÃ© si donnÃ©es KO.
3. **Conteneuriser les jobs** â†’ reproductibles.
4. **Automatiser les dÃ©ploiements** (CI/CD).
5. **Surveiller la donnÃ©e et non seulement le code**.
6. **Infra as Code** â†’ cluster/data lake crÃ©Ã©s automatiquement.
7. **Collaboration** â†’ Dev + Data Engineer + Analyst + Ops + ML eng.

---

## 12. ğŸ“ˆ Exemple dâ€™architecture DataOps moderne

1. **Ingestion** : Kafka / Fivetran / API.
2. **Stockage** : S3 Data Lake + Snowflake/BigQuery.
3. **Transformation** : dbt + Spark.
4. **Orchestration** : Airflow.
5. **Tests Data** : Great Expectations.
6. **CI/CD** : GitHub Actions (dÃ©ploiement DAGs + dbt).
7. **Monitoring** : Prometheus + Grafana + Slack alerts.

---

## 13. ğŸ§­ Roadmap dâ€™apprentissage DevOps pour la Data

1. Git + GitHub Actions (CI/CD).
2. Docker (packager un job ETL).
3. Airflow (orchestrer pipelines).
4. Great Expectations (tests data).
5. Terraform (provisionner S3/BigQuery).
6. Monitoring (Prometheus/Grafana).
7. Kubernetes (scalabilitÃ© Spark, Airflow, Kafka).

---
